{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Anomaly detection\n",
    "\n",
    "## Introduction\n",
    "\n",
    "### Definition of the problem\n",
    "\n",
    "Given a data set $\\{x^{(1)}, x{^(2)}, ..., x^{(m)}\\}$, we consider $x_{test}$ anomalous if the probability of $x_{test}$ \n",
    "being seen in the dataset $p(x_{test}) < \\epsilon$, where $\\epsilon$ is a very small number.\n",
    "\n",
    "### Density estimation\n",
    "\n",
    "Training set: $\\{x^{(1)}, x{^(2)}, ..., x^{(m)}\\}$\n",
    "Each $x^{(i)}$ has n features\n",
    "\n",
    "Each feature is assumed to have a normal distribution. The features are assumed to be independent from each other.\n",
    "\n",
    "$$p(x) = p(x_1; \\mu_1, \\sigma^2_1)*p(x_2; \\mu_2, \\sigma^2_2)*p(x_3; \\mu_3, \\sigma^2_3)*... *p(x_n;\\mu_n, \\sigma^2_n) = \\prod_{j = 1}^n p(x_j; \\mu_j, \\sigma_j^2)$$\n",
    "\n",
    "## Algorithm\n",
    "\n",
    "<pre>\n",
    "   Fit parameters &mu;<sub>1</sub>, ..., &mu;<sub>n</sub>, &sigma;<sub>1</sub><sup>2</sup>, ..., &sigma;<sub>n</sub><sup>2</sup>\n",
    "   Given a new example x, compute p(x):\n",
    "</pre>\n",
    "\n",
    "$$p(x) = \\prod_{j = 1}^n p(x_j; \\mu_j, \\sigma_j^2) = \\prod_{j = 1}^n \\frac{1}{\\sqrt{2\\pi\\sigma_j}}exp(-\\frac{(x_j-\\mu_j)^2}{2\\sigma_j^2})$$\n",
    "\n",
    "<pre>\n",
    "   Anomaly if p(x) < &epsilon;\n",
    "</pre>\n",
    "\n",
    "## Evaluating an anomaly detection algorithm\n",
    "\n",
    "Assume we have some labeled data, of anomalous (a few) and non-anomalous examples.\n",
    "\n",
    "We will prepare:\n",
    "\n",
    "1. Training set: $\\{x^{(1)}, x{^(2)}, ..., x^{(m)}\\}$ We assume all are normal, non-anomalous examples (not a problem if in reality some are not)\n",
    "2. Cross validation set: $\\{x_{cv}^{(1)}, x_{cv}{^(2)}, ..., x_{cv}^{(m_{cv})}\\}$, $\\{y_{cv}^{(1)}, y_{cv}{^(2)}, ..., y_{cv}^{(m_{cv})}\\}$ \n",
    "3. Test validation set: $\\{x_{test}^{(1)}, x_{test}{^(2)}, ..., x_{test}^{(m_{test})}\\}$, $\\{y_{test}^{(1)}, y_{test}{^(2)}, ..., y_{test}^{(m_{test})}\\}$ \n",
    "\n",
    "The cross validation and test sets will have some anomalies (i.e, data points with y=1)\n",
    "\n",
    "Alternatively we could only use a training set and a cross validation set, for example if we don't have enough examples of anomalies. \n",
    "\n",
    "The algorithm will predict:\n",
    "\n",
    "$$\n",
    "y = \\begin{cases}\n",
    "      1,  \\text{if $p(x) < \\epsilon$}\\\\\n",
    "      0,  \\text{if $p(x) >= \\epsilon$}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "Then you can use a metric like F<sub>1</sub> score to choose the parameter &epsilon;.\n",
    "\n",
    "## When to use anomaly detection vs supervised learning\n",
    "\n",
    "Use anomaly detection if:\n",
    "\n",
    "1. You have a very small number of positive examples (y=1) (0-20 is common), and\n",
    "2. There are many different \"types\"of anomalies so it is hard for any algorithm to learn from positive examples how anomalies look like.\n",
    "\n",
    "Use supervise learning if:\n",
    "\n",
    "1. You have a (relatively) big number of positive examples,that can let the algorithm get a sense of what positive examples are like.\n",
    "\n",
    "## Transforming the features\n",
    "\n",
    "If the features are non-gaussian they can be transformed, for example, applying a logarithm $log(x_2 + c)$ or root $x_3^{1/3}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
