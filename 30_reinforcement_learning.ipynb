{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement learning\n",
    "\n",
    "## Description of the problem\n",
    "\n",
    "An **entity** can be is a **state** $s$. Each state has associated a **reward** $R(s)$. From that state it can take **actions** $a$ that will take the entity to another state $s'$. The state, its reward, an action that can be taken in that state, and the state the entity will get to with that action can be represented with a tuple:\n",
    "\n",
    "$$(s, a, R(s), s')$$\n",
    "\n",
    "**Return** is the sum of the rewards from a sequence of states and actions, wieghted by a discount factor that compounds along the series of actions. \n",
    "\n",
    "$$return = R(s) + \\gamma R(s') + \\gamma^2 R(s'') + \\dots$$\n",
    "\n",
    "A **Policy** is a function $\\Pi$ that for a given state gives us the action $a$ to take next.\n",
    "\n",
    "The objective of **reinforcement learning algorithm** is to find a policy $\\Pi(s) = a$ that maximizes the return.\n",
    "\n",
    "This problem formulation is a **Markov Decision Process (MDP)**. In a MDP, the future only depends on the current state, regardless of how we have gotten to that state.\n",
    "\n",
    "## State action value function\n",
    "\n",
    "We define the **state action value function** (sometimes also called *Q-function*) $Q(s, a)$ is the return if you,\n",
    "* start in state $s$\n",
    "* take action $a$\n",
    "* behave optimally after that\n",
    "\n",
    "The best possible return from state s is $\\max_{a} Q(s,a)$\n",
    "\n",
    "## Bellman equation\n",
    "\n",
    "$$Q(s, a) = R(s) + \\gamma \\max_{a'} Q(s', a')$$\n",
    "\n",
    "Where $a'$ is each of the actions that can be taken in state $s'$\n",
    "\n",
    "## Random (stochastic) environment\n",
    "\n",
    "In this case the return of a sequence of states is\n",
    "\n",
    "return = $$E[R(s) + \\gamma R(s') + \\gamma^2 R(s'') + \\dots ]$$\n",
    "\n",
    "And the Belmman equation is:\n",
    "\n",
    "$$Q(s,a) = R(s) + \\gamma E[\\max_{a'} Q(s', a')]$$\n",
    "\n",
    "## Continuous state spaces\n",
    "\n",
    "Example: state space for a car, in a 2-D world,\n",
    "\n",
    "$$\\begin{bmatrix}x \\\\ y \\\\ \\theta \\\\ \\dot{x} \\\\ \\dot{y} \\\\ \\dot{\\theta}\\end{bmatrix}$$\n",
    "\n",
    "where $x$ and $y$ are the coordinates, $\\theta$ is the orientation (angle), and $\\dot{x}$, $\\dot{y}$, $\\dot{\\theta}$ the rate of change along each coordinate and the orientation.\n",
    "\n",
    "## Learning the state action value function\n",
    "\n",
    "Idea: Train a neural network to calculate, for a given state $s$, the return of the state action value functions for the actions possible in that state, so we can choose the one with largest $Q(s, a)$. In other words, train a neural network that given s and a returns $y \\approx Q(a,a)$. Or, in less words, train the neural network to learn the Bellman equation.\n",
    "\n",
    "To do so, we can create a large set of tuples\n",
    "\n",
    "$$(s^{(1)}, a^{(1)}, R(s^{(1)}), s'^{(1)}) \\\\ (s^{(2)}, a^{(2)}, R(s^{(2)}), s'^{(2)}) \\\\ \\dots$$\n",
    "\n",
    "And then, the training examples for the neural network will be:\n",
    "\n",
    "* For the inputs $x$, each of the tuples \n",
    "$$(s^{(1)}, a^{(1)}), (s^{(2)}, a^{(2)}), \\dots$$\n",
    "* For the target values y, the corresponding \n",
    "$$Q(s^{(1)},a^{(1)}), Q(s^{(2)},a^{(2)}), \\dots$$\n",
    "\n",
    "calculated with the Bellman equation, for example\n",
    "\n",
    "$$Q(s^{(1)}, a^{(1)}) = R(s^{(1)}) + \\gamma \\max_{a'} Q(s'^{(1)}, a')$$\n",
    "\n",
    "Note that the target values $y$ depend only on the last two elements of the tuples $(s^{(i)}, a^{(i)}, R(s^{(i)}), s'^{(i)})$\n",
    "\n",
    "At the begining, we don't know the $Q(s, a)$ function, but it can be initialized randomly. In every step, it will get better.\n",
    "\n",
    "Learning algorithm (sometimes call the **Deep-Q network**)\n",
    "\n",
    "<pre>\n",
    "    Initialize Q_nn neural network randomly as guess of Q(s, a)\n",
    "    Repeat {\n",
    "        Take actions to generate tuples (s, a, R(s), s')\n",
    "        Store the 10,000 more recent examples of these tuples (replay buffer)\n",
    "        Train neural network:\n",
    "            Create training set of 10,000 examples x, y using\n",
    "                x = (s, a) and y = R(s) + &gamma; max<sub>a'</sub> Q_nn(s', a')\n",
    "            Train Q<sub>new</sub> such that Q<sub>new</sub>(s, a) &asymp; y\n",
    "        Set Q_nn = Q<sub>new</sub>\n",
    "    }\n",
    "</pre>\n",
    "\n",
    "> Note: It is not clear in the lecture, but wonder if the \"take actions to generate tuples\" in the lesson means take sequence of actions until you reach a final state. Refer to the ideas in the \"Search\" chapter of the *CS50: AI with Python* course in edX. Maybe not, since here we are just trying to generate training samples to calculate $Q(s, a)$\n",
    "\n",
    "One possible architecture of the neural network is (from course example for lunar lander, with 8 parameters for the state,and 4 possible actions, one hot encoded):\n",
    "\n",
    "```pyhton\n",
    "tf.keras.models.Sequential ([\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dense(1)\n",
    "]) \n",
    "```\n",
    "\n",
    "An improved architecture uses (for this case) four units in the output layer, to compute at the same time the $Q(s, a)$ function for all the possible actions in one state. The input, in this case, is the 8 parameters that represent the state.\n",
    "\n",
    "```pyhton\n",
    "tf.keras.models.Sequential ([\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dense(4)\n",
    "]) \n",
    "```\n",
    "\n",
    "## Algorithm refinement: $\\epsilon$-greedy policy\n",
    "\n",
    "How to improve the \"`Take actions to generate tuples (s, a, R(s), a')`\" step in the algorithm?\n",
    "\n",
    "> Note to self: again, refer to the \"Search\" chapter of the *CS50: AI with Python* course in edX.\n",
    "\n",
    "Instead of taking the actions randomly, use the following algorithm,\n",
    "\n",
    "<pre>\n",
    "    With probability (1- &epsilon;) pick the action a that maximizes Q(s, a)\n",
    "    With probability &epsilon; pick an action a randomly\n",
    "</pre>\n",
    "\n",
    "## Algorithm refinement: mini-batch and soft updates\n",
    "\n",
    "### Mini-bacthes\n",
    "\n",
    "This refinement also applies to linear regression or the training of a neural network.\n",
    "\n",
    "Idea: instead of using all the samples to calculate the cost function in each step of the gradient decent algorithm, do it using a subset (*batch*) of the trainign examples (e.g., with a training set of 1,000,000, use a batch of 1,000 examples).\n",
    "\n",
    "### Soft updates\n",
    "\n",
    "The last step in the algorithm was to replace the $Q(s, a)$ function with the newly calculated $Q_{new}(s, a)$. Doing so, it can create abrupt changes in the Q function, sometings replacing a somehow good function by a worse one.\n",
    "\n",
    "The idea is instead of replacing the parameters of the newural network with the new ones, replace them so that:\n",
    "\n",
    "$$\n",
    "   W = 0.01 W_{new} + 0.99 W \\\\\n",
    "   B = 0.01 B_{new} + 0.99 B\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-11 13:20:48.548244: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX512_VNNI\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "STATE_FEATURES = 1\n",
    "ACTIONS = [0, 1, 2, 3]\n",
    "EPSILON = 0.01\n",
    "GAMMA = 0.98\n",
    "\n",
    "#ITERATIONS = 10000\n",
    "#UPDATE_STEP = 500\n",
    "#TRAIN_SAMPLES = 100\n",
    "ITERATIONS = 50\n",
    "UPDATE_STEP = 10\n",
    "TRAIN_SAMPLES = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_NN():\n",
    "    \"\"\"\n",
    "    Defines and compiles the neural network\n",
    "    \"\"\"\n",
    "    nnetwork = tf.keras.models.Sequential ([\n",
    "        tf.keras.layers.Input(shape=STATE_FEATURES),\n",
    "        tf.keras.layers.Dense(64, activation=\"relu\"),\n",
    "        tf.keras.layers.Dense(64, activation=\"relu\"),\n",
    "        tf.keras.layers.Dense(len(ACTIONS))\n",
    "    ])\n",
    "\n",
    "    nnetwork.compile(\n",
    "        optimizer=keras.optimizers.Adam(learning_rate=0.01),\n",
    "        loss=tf.keras.losses.MeanSquaredError()\n",
    "    )\n",
    "\n",
    "    return nnetwork"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def egreedy_select(state, qnetwork, epsilon):\n",
    "    \"\"\"\n",
    "    Given a state and a state action value function (implemented with a q-network) selects an\n",
    "    action implementing the e-greedy policy\n",
    "\n",
    "    Arguments:\n",
    "        state:\n",
    "        qnetwork:\n",
    "        epsilon:\n",
    "\n",
    "    Returns:\n",
    "        action:\n",
    "    \"\"\"\n",
    "    \n",
    "    if np.random.choice([True, False], p=[1 - epsilon, epsilon]):\n",
    "        # Here the greedy selection of next action\n",
    "        # Transform dimensions of state for qnetwork\n",
    "        state_qn = np.expand_dims(state, axis=0)\n",
    "        q_values = qnetwork(state_qn).numpy()\n",
    "        if np.unique(q_values).shape[0] == 1:\n",
    "            # All q_values are the same, we do a random select\n",
    "            action = np.random.choice(ACTIONS)\n",
    "        else:   \n",
    "            # Selection of the action with max q_value\n",
    "            action = q_values.argmax(axis=1)\n",
    "    else:\n",
    "        action = np.random.choice(ACTIONS)\n",
    "    \n",
    "    return int(action)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-11 13:21:00.501917: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX512_VNNI\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-03-11 13:21:00.527996: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: \n"
     ]
    }
   ],
   "source": [
    "q_nn = create_NN()\n",
    "q_new = create_NN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "env = gym.make('FrozenLake-v1', desc=None, map_name=\"8x8\", is_slippery=True, render_mode=\"human\")\n",
    "state, _ = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# buffer to store the tupples (state, action, reward, state_prime)\n",
    "buffer = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Exception encountered when calling layer 'sequential' (type Sequential).\n\nInput 0 of layer \"dense\" is incompatible with the layer: expected axis -1 of input shape to have value 1, but received input with shape (1, 2)\n\nCall arguments received by layer 'sequential' (type Sequential):\n  • inputs=tf.Tensor(shape=(1, 2), dtype=float64)\n  • training=None\n  • mask=None",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/home/damian/learning/mlspecialization/my_summary/30_reinforcement_learning.ipynb Cell 9\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/damian/learning/mlspecialization/my_summary/30_reinforcement_learning.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=20'>21</a>\u001b[0m X_train \u001b[39m=\u001b[39m train_samples[:,\u001b[39m0\u001b[39m]\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/damian/learning/mlspecialization/my_summary/30_reinforcement_learning.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=21'>22</a>\u001b[0m \u001b[39m# y_train is a 2-D array, with the state action value calculated for each possible\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/damian/learning/mlspecialization/my_summary/30_reinforcement_learning.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=22'>23</a>\u001b[0m \u001b[39m# action in state_prime. This works thanks to broadcasting\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell://wsl%2Bubuntu/home/damian/learning/mlspecialization/my_summary/30_reinforcement_learning.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=23'>24</a>\u001b[0m y_train \u001b[39m=\u001b[39m train_samples[:,\u001b[39m2\u001b[39m] \u001b[39m+\u001b[39m GAMMA\u001b[39m*\u001b[39mq_nn(np\u001b[39m.\u001b[39;49mexpand_dims(train_samples[:,\u001b[39m3\u001b[39;49m], axis\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m))\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/damian/learning/mlspecialization/my_summary/30_reinforcement_learning.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=24'>25</a>\u001b[0m \u001b[39mprint\u001b[39m(X_train)\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/damian/learning/mlspecialization/my_summary/30_reinforcement_learning.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=25'>26</a>\u001b[0m \u001b[39mprint\u001b[39m(y_train)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/keras/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[1;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/keras/engine/input_spec.py:277\u001b[0m, in \u001b[0;36massert_input_compatibility\u001b[0;34m(input_spec, inputs, layer_name)\u001b[0m\n\u001b[1;32m    272\u001b[0m             value \u001b[39m=\u001b[39m value\u001b[39m.\u001b[39mvalue\n\u001b[1;32m    273\u001b[0m         \u001b[39mif\u001b[39;00m value \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m shape_as_list[\u001b[39mint\u001b[39m(axis)] \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m {\n\u001b[1;32m    274\u001b[0m             value,\n\u001b[1;32m    275\u001b[0m             \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    276\u001b[0m         }:\n\u001b[0;32m--> 277\u001b[0m             \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    278\u001b[0m                 \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mInput \u001b[39m\u001b[39m{\u001b[39;00minput_index\u001b[39m}\u001b[39;00m\u001b[39m of layer \u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mlayer_name\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m is \u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m    279\u001b[0m                 \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mincompatible with the layer: expected axis \u001b[39m\u001b[39m{\u001b[39;00maxis\u001b[39m}\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    280\u001b[0m                 \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mof input shape to have value \u001b[39m\u001b[39m{\u001b[39;00mvalue\u001b[39m}\u001b[39;00m\u001b[39m, \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    281\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39mbut received input with \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    282\u001b[0m                 \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mshape \u001b[39m\u001b[39m{\u001b[39;00mdisplay_shape(x\u001b[39m.\u001b[39mshape)\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    283\u001b[0m             )\n\u001b[1;32m    284\u001b[0m \u001b[39m# Check shape.\u001b[39;00m\n\u001b[1;32m    285\u001b[0m \u001b[39mif\u001b[39;00m spec\u001b[39m.\u001b[39mshape \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m shape\u001b[39m.\u001b[39mrank \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[0;31mValueError\u001b[0m: Exception encountered when calling layer 'sequential' (type Sequential).\n\nInput 0 of layer \"dense\" is incompatible with the layer: expected axis -1 of input shape to have value 1, but received input with shape (1, 2)\n\nCall arguments received by layer 'sequential' (type Sequential):\n  • inputs=tf.Tensor(shape=(1, 2), dtype=float64)\n  • training=None\n  • mask=None"
     ]
    }
   ],
   "source": [
    "for i in range(ITERATIONS):\n",
    "    print(state)\n",
    "    # Transform dimensions of state for qnetwork\n",
    "    state_qn = np.expand_dims(state, axis=0)\n",
    "    action = egreedy_select(state, q_nn, EPSILON)\n",
    "    state_prime, reward, done, _, _ = env.step(action)\n",
    "\n",
    "    env.render()\n",
    "    buffer.append((state, action, reward, state_prime))\n",
    "    if done:\n",
    "        # Go back to initial state\n",
    "        state, _ = env.reset()\n",
    "    else:\n",
    "        state = state_prime\n",
    "\n",
    "    if i % UPDATE_STEP:\n",
    "        # Train the network\n",
    "        # Numpy array with the sample tuples as rows (state, action, reward, state_prime)\n",
    "        train_samples = np.array(random.sample(buffer,k=TRAIN_SAMPLES))\n",
    "        # The inputs are the states\n",
    "        X_train = train_samples[:,0]\n",
    "        # y_train is a 2-D array, with the state action value calculated for each possible\n",
    "        # action in state_prime. This works thanks to broadcasting\n",
    "        y_train = train_samples[:,2] + GAMMA*q_nn(np.expand_dims(train_samples[:,3], axis=0))\n",
    "        print(X_train)\n",
    "        print(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Layer \"sequential\" expects 1 input(s), but it received 2 input tensors. Inputs received: [<tf.Tensor: shape=(), dtype=int32, numpy=5>, <tf.Tensor: shape=(), dtype=int32, numpy=6>]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/home/damian/learning/mlspecialization/my_summary/30_reinforcement_learning.ipynb Cell 10\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/home/damian/learning/mlspecialization/my_summary/30_reinforcement_learning.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m state \u001b[39m=\u001b[39m [[(\u001b[39m5\u001b[39m,)], [(\u001b[39m6\u001b[39m,)]]\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/home/damian/learning/mlspecialization/my_summary/30_reinforcement_learning.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39m#state_qn = np.expand_dims(state, axis=0)\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell://wsl%2Bubuntu/home/damian/learning/mlspecialization/my_summary/30_reinforcement_learning.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m q_values \u001b[39m=\u001b[39m q_nn(state)\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/home/damian/learning/mlspecialization/my_summary/30_reinforcement_learning.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mprint\u001b[39m(q_values)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/keras/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[1;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/keras/engine/input_spec.py:216\u001b[0m, in \u001b[0;36massert_input_compatibility\u001b[0;34m(input_spec, inputs, layer_name)\u001b[0m\n\u001b[1;32m    213\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mInputs to a layer should be tensors. Got: \u001b[39m\u001b[39m{\u001b[39;00mx\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    215\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(inputs) \u001b[39m!=\u001b[39m \u001b[39mlen\u001b[39m(input_spec):\n\u001b[0;32m--> 216\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    217\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mLayer \u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mlayer_name\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m expects \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlen\u001b[39m(input_spec)\u001b[39m}\u001b[39;00m\u001b[39m input(s),\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m    218\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m but it received \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlen\u001b[39m(inputs)\u001b[39m}\u001b[39;00m\u001b[39m input tensors. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    219\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mInputs received: \u001b[39m\u001b[39m{\u001b[39;00minputs\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    220\u001b[0m     )\n\u001b[1;32m    221\u001b[0m \u001b[39mfor\u001b[39;00m input_index, (x, spec) \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(\u001b[39mzip\u001b[39m(inputs, input_spec)):\n\u001b[1;32m    222\u001b[0m     \u001b[39mif\u001b[39;00m spec \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[0;31mValueError\u001b[0m: Layer \"sequential\" expects 1 input(s), but it received 2 input tensors. Inputs received: [<tf.Tensor: shape=(), dtype=int32, numpy=5>, <tf.Tensor: shape=(), dtype=int32, numpy=6>]"
     ]
    }
   ],
   "source": [
    "\n",
    "state = [[(5,)], [(6,)]]\n",
    "#state_qn = np.expand_dims(state, axis=0)\n",
    "q_values = q_nn(state)\n",
    "print(q_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "test_egreedy_select1 (__main__.TestNotebook)\n",
      "Test selection of the state with highest state action value ... ok\n",
      "test_egreedy_select2 (__main__.TestNotebook)\n",
      "Test random selection of the state when all state action values are the same ... ok\n",
      "test_egreedy_select3 (__main__.TestNotebook)\n",
      "Test random selection with epsilon <> 0 ... ok\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 3 tests in 0.535s\n",
      "\n",
      "OK\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<unittest.main.TestProgram at 0x7fbedd1f6070>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Unit tests\n",
    "\n",
    "import unittest \n",
    "\n",
    "class TestNotebook(unittest.TestCase):\n",
    "\n",
    "    def test_egreedy_select1(self):\n",
    "        '''\n",
    "        Test selection of the state with highest state action value\n",
    "        '''\n",
    "        nn = tf.keras.models.Sequential([\n",
    "            tf.keras.layers.Input(shape=STATE_FEATURES),\n",
    "            tf.keras.layers.Dense(len(ACTIONS))\n",
    "        ])\n",
    "\n",
    "        nn.compile(\n",
    "            optimizer=keras.optimizers.Adam(learning_rate=0.01),\n",
    "            loss=tf.keras.losses.MeanSquaredError()\n",
    "        )\n",
    "\n",
    "        # State is irrelevant here\n",
    "        state = 5\n",
    "\n",
    "        # Set weights of nn so action 2 has the highest state action value \n",
    "        weights = np.array([np.array([[0., 1., 2., 1.]]), np.array([0., 0., 0. ,0.])], dtype='object')\n",
    "        nn.set_weights(weights)\n",
    "        \n",
    "        self.assertEqual(egreedy_select(state, nn, epsilon=0), 2)\n",
    "\n",
    "    def test_egreedy_select2(self):\n",
    "        '''\n",
    "        Test random selection of the state when all state action values are the same\n",
    "        '''\n",
    "        nn = tf.keras.models.Sequential([\n",
    "            tf.keras.layers.Input(shape=STATE_FEATURES),\n",
    "            tf.keras.layers.Dense(len(ACTIONS))\n",
    "        ])\n",
    "\n",
    "        nn.compile(\n",
    "            optimizer=keras.optimizers.Adam(learning_rate=0.01),\n",
    "            loss=tf.keras.losses.MeanSquaredError()\n",
    "        )\n",
    "\n",
    "        # State is irrelevant here\n",
    "        state = 5\n",
    "\n",
    "        # Set weights \n",
    "        weights = np.array([np.array([[1., 1., 1., 1.]]), np.array([0., 0., 0. ,0.])], dtype='object')\n",
    "        nn.set_weights(weights)\n",
    "        \n",
    "        np.random.seed(66)\n",
    "        self.assertEqual(egreedy_select(state, nn, epsilon=0), 3)\n",
    "\n",
    "    def test_egreedy_select3(self):\n",
    "        '''\n",
    "        Test random selection with epsilon <> 0\n",
    "        '''\n",
    "        nn = tf.keras.models.Sequential([\n",
    "            tf.keras.layers.Input(shape=STATE_FEATURES),\n",
    "            tf.keras.layers.Dense(len(ACTIONS))\n",
    "        ])\n",
    "\n",
    "        nn.compile(\n",
    "            optimizer=keras.optimizers.Adam(learning_rate=0.01),\n",
    "            loss=tf.keras.losses.MeanSquaredError()\n",
    "        )\n",
    "\n",
    "        # State is irrelevant here\n",
    "        state = 5\n",
    "\n",
    "        # Set weights \n",
    "        weights = np.array([np.array([[0., 1., 2., 3.]]), np.array([0., 0., 0. ,0.])], dtype='object')\n",
    "        nn.set_weights(weights)\n",
    "        \n",
    "        np.random.seed(66)\n",
    "        self.assertEqual(egreedy_select(state, nn, epsilon=1), 3)\n",
    "\n",
    "unittest.main(argv=[''], verbosity=2, exit=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
