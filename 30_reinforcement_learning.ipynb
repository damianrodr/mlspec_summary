{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement learning\n",
    "\n",
    "## Description of the problem\n",
    "\n",
    "An **entity** can be is a **state** $s$. Each state has associated a **reward** $R(s)$. From that state it can take **actions** $a$ that will take the entity to another state $s'$. The state, its reward, an action that can be taken in that state, and the state the entity will get to with that action can be represented with a tuple:\n",
    "\n",
    "$$(s, a, R(s), s')$$\n",
    "\n",
    "**Return** is the sum of the rewards from a sequence of states and actions, wieghted by a discount factor that compounds along the series of actions. \n",
    "\n",
    "$$return = R(s) + \\gamma R(s') + \\gamma^2 R(s'') + \\dots$$\n",
    "\n",
    "A **Policy** is a function $\\Pi$ that for a given state gives us the action $a$ to take next.\n",
    "\n",
    "The objective of **reinforcement learning algorithm** is to find a policy $\\Pi(s) = a$ that maximizes the return.\n",
    "\n",
    "This problem formulation is a **Markov Decision Process (MDP)**. In a MDP, the future only depends on the current state, regardless of how we have gotten to that state.\n",
    "\n",
    "## State action value function\n",
    "\n",
    "We define the **state action value function** (sometimes also called *Q-function*) $Q(s, a)$ is the return if you,\n",
    "* start in state $s$\n",
    "* take action $a$\n",
    "* behave optimally after that\n",
    "\n",
    "The best possible return from state s is $\\max_{a} Q(s,a)$\n",
    "\n",
    "## Bellman equation\n",
    "\n",
    "$$Q(s, a) = R(s) + \\gamma \\max_{a'} Q(s', a')$$\n",
    "\n",
    "Where $a'$ is each of the actions that can be taken in state $s'$\n",
    "\n",
    "## Random (stochastic) environment\n",
    "\n",
    "In this case the return of a sequence of states is\n",
    "\n",
    "return = $$E[R(s) + \\gamma R(s') + \\gamma^2 R(s'') + \\dots ]$$\n",
    "\n",
    "And the Belmman equation is:\n",
    "\n",
    "$$Q(s,a) = R(s) + \\gamma E[\\max_{a'} Q(s', a')]$$\n",
    "\n",
    "## Continuous state spaces\n",
    "\n",
    "Example: state space for a car, in a 2-D world,\n",
    "\n",
    "$$\\begin{bmatrix}x \\\\ y \\\\ \\theta \\\\ \\dot{x} \\\\ \\dot{y} \\\\ \\dot{\\theta}\\end{bmatrix}$$\n",
    "\n",
    "where $x$ and $y$ are the coordinates, $\\theta$ is the orientation (angle), and $\\dot{x}$, $\\dot{y}$, $\\dot{\\theta}$ the rate of change along each coordinate and the orientation.\n",
    "\n",
    "## Learning the state action value function\n",
    "\n",
    "Idea: Train a neural network to calculate, for a given state $s$, the return of the state action value functions for the actions possible in that state, so we can choose the one with largest $Q(s, a)$. In other words, train a neural network that given s and a returns $y \\approx Q(a,a)$. Or, in less words, train the neural network to learn the Bellman equation.\n",
    "\n",
    "To do so, we can create a large set of tuples\n",
    "\n",
    "$$(s^{(1)}, a^{(1)}, R(s^{(1)}), s'^{(1)}) \\\\ (s^{(2)}, a^{(2)}, R(s^{(2)}), s'^{(2)}) \\\\ \\dots$$\n",
    "\n",
    "And then, the training examples for the neural network will be:\n",
    "\n",
    "* For the inputs $x$, each of the tuples \n",
    "$$(s^{(1)}, a^{(1)}), (s^{(2)}, a^{(2)}), \\dots$$\n",
    "* For the target values y, the corresponding \n",
    "$$Q(s^{(1)},a^{(1)}), Q(s^{(2)},a^{(2)}), \\dots$$\n",
    "\n",
    "calculated with the Bellman equation, for example\n",
    "\n",
    "$$Q(s^{(1)}, a^{(1)}) = R(s^{(1)}) + \\gamma \\max_{a'} Q(s'^{(1)}, a')$$\n",
    "\n",
    "Note that the target values $y$ depend only on the last two elements of the tuples $(s^{(i)}, a^{(i)}, R(s^{(i)}), s'^{(i)})$\n",
    "\n",
    "At the begining, we don't know the $Q(s, a)$ function, but it can be initialized randomly. In every step, it will get better.\n",
    "\n",
    "Learning algorithm (sometimes call the **Deep-Q network**)\n",
    "\n",
    "<pre>\n",
    "    Initialize Q_nn neural network randomly as guess of Q(s, a)\n",
    "    Repeat {\n",
    "        Take actions to generate tuples (s, a, R(s), s')\n",
    "        Store the 10,000 more recent examples of these tuples (replay buffer)\n",
    "        Train neural network:\n",
    "            Create training set of 10,000 examples x, y using\n",
    "                x = (s, a) and y = R(s) + &gamma; max<sub>a'</sub> Q_nn(s', a')\n",
    "            Train Q<sub>new</sub> such that Q<sub>new</sub>(s, a) &asymp; y\n",
    "        Set Q_nn = Q<sub>new</sub>\n",
    "    }\n",
    "</pre>\n",
    "\n",
    "> Note: It is not clear in the lecture, but wonder if the \"take actions to generate tuples\" in the lesson means take sequence of actions until you reach a final state. Refer to the ideas in the \"Search\" chapter of the *CS50: AI with Python* course in edX. Maybe not, since here we are just trying to generate training samples to calculate $Q(s, a)$\n",
    "\n",
    "One possible architecture of the neural network is (from course example for lunar lander, with 8 parameters for the state,and 4 possible actions, one hot encoded):\n",
    "\n",
    "```pyhton\n",
    "tf.keras.models.Sequential ([\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dense(1)\n",
    "]) \n",
    "```\n",
    "\n",
    "An improved architecture uses (for this case) four units in the output layer, to compute at the same time the $Q(s, a)$ function for all the possible actions in one state. The input, in this case, is the 8 parameters that represent the state.\n",
    "\n",
    "```pyhton\n",
    "tf.keras.models.Sequential ([\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dense(4)\n",
    "]) \n",
    "```\n",
    "\n",
    "## Algorithm refinement: $\\epsilon$-greedy policy\n",
    "\n",
    "How to improve the \"`Take actions to generate tuples (s, a, R(s), a')`\" step in the algorithm?\n",
    "\n",
    "> Note to self: again, refer to the \"Search\" chapter of the *CS50: AI with Python* course in edX.\n",
    "\n",
    "Instead of taking the actions randomly, use the following algorithm,\n",
    "\n",
    "<pre>\n",
    "    With probability (1- &epsilon;) pick the action a that maximizes Q(s, a)\n",
    "    With probability &epsilon; pick an action a randomly\n",
    "</pre>\n",
    "\n",
    "## Algorithm refinement: mini-batch and soft updates\n",
    "\n",
    "### Mini-bacthes\n",
    "\n",
    "This refinement also applies to linear regression or the training of a neural network.\n",
    "\n",
    "Idea: instead of using all the samples to calculate the cost function in each step of the gradient decent algorithm, do it using a subset (*batch*) of the trainign examples (e.g., with a training set of 1,000,000, use a batch of 1,000 examples).\n",
    "\n",
    "### Soft updates\n",
    "\n",
    "The last step in the algorithm was to replace the $Q(s, a)$ function with the newly calculated $Q_{new}(s, a)$. Doing so, it can create abrupt changes in the Q function, sometings replacing a somehow good function by a worse one.\n",
    "\n",
    "The idea is instead of replacing the parameters of the newural network with the new ones, replace them so that:\n",
    "\n",
    "$$\n",
    "   W = 0.01 W_{new} + 0.99 W \\\\\n",
    "   B = 0.01 B_{new} + 0.99 B\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-11 22:55:53.265925: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX512_VNNI\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from collections import deque, namedtuple\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "STATE_FEATURES = 1\n",
    "ACTIONS = [0, 1, 2, 3]\n",
    "EPSILON = 0.01\n",
    "GAMMA = 0.98\n",
    "ALPHA = 0.001\n",
    "TAU = 0.001\n",
    "\n",
    "#ITERATIONS = 10000\n",
    "#UPDATE_STEP = 500\n",
    "#TRAIN_SAMPLES = 100\n",
    "ITERATIONS = 500\n",
    "UPDATE_STEP = 20\n",
    "TRAIN_SAMPLES = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_NN():\n",
    "    \"\"\"\n",
    "    Defines and compiles the neural network\n",
    "    \"\"\"\n",
    "    nnetwork = tf.keras.models.Sequential ([\n",
    "        tf.keras.layers.Input(shape=STATE_FEATURES),\n",
    "        tf.keras.layers.Dense(64, activation=\"relu\"),\n",
    "        tf.keras.layers.Dense(64, activation=\"relu\"),\n",
    "        tf.keras.layers.Dense(len(ACTIONS))\n",
    "    ])\n",
    "\n",
    "    nnetwork.compile(\n",
    "        optimizer=keras.optimizers.Adam(learning_rate=ALPHA),\n",
    "        loss=tf.keras.losses.MeanSquaredError()\n",
    "    )\n",
    "\n",
    "    return nnetwork"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def egreedy_select(state, qnetwork, epsilon):\n",
    "    \"\"\"\n",
    "    Given a state and a state action value function (implemented with a q-network) selects an\n",
    "    action implementing the e-greedy policy\n",
    "\n",
    "    Arguments:\n",
    "        state:\n",
    "        qnetwork:\n",
    "        epsilon:\n",
    "\n",
    "    Returns:\n",
    "        action:\n",
    "    \"\"\"\n",
    "    \n",
    "    if np.random.choice([True, False], p=[1 - epsilon, epsilon]):\n",
    "        # Here the greedy selection of next action\n",
    "        # Transform dimensions of state for qnetwork\n",
    "        state_qn = np.expand_dims(state, axis=0)\n",
    "        q_values = qnetwork(state_qn).numpy()\n",
    "        if np.unique(q_values).shape[0] == 1:\n",
    "            # All q_values are the same, we do a random select\n",
    "            action = np.random.choice(ACTIONS)\n",
    "        else:   \n",
    "            # Selection of the action with max q_value\n",
    "            action = q_values.argmax(axis=1)\n",
    "    else:\n",
    "        action = np.random.choice(ACTIONS)\n",
    "    \n",
    "    return int(action)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss(samples, q_new, q_nn, gamma):\n",
    "\n",
    "    states = tf.convert_to_tensor(\n",
    "        np.array([s.state for s in samples if s is not None]), dtype=tf.float32\n",
    "    )\n",
    "    actions = tf.convert_to_tensor(\n",
    "        np.array([s.action for s in samples if s is not None]), dtype=tf.float32\n",
    "    )\n",
    "    rewards = tf.convert_to_tensor(\n",
    "        np.array([s.reward for s in samples if s is not None]), dtype=tf.float32\n",
    "    )\n",
    "    states_prime = tf.convert_to_tensor(\n",
    "        np.array([s.state_prime for s in samples if s is not None]), dtype=tf.float32\n",
    "    )\n",
    "    done_values = tf.convert_to_tensor(\n",
    "        np.array([s.done for s in samples if s is not None]), dtype=tf.float32\n",
    "    )\n",
    "\n",
    "    # Bellman equation\n",
    "    qvalues_target = rewards + gamma * (1 - done_values) * tf.reduce_max(q_nn(states_prime), axis=-1)\n",
    "\n",
    "    # Get the q_values\n",
    "    q_values = q_new(states)\n",
    "    q_values = tf.gather_nd(q_values, tf.stack([tf.range(q_values.shape[0]),\n",
    "                                                tf.cast(actions, tf.int32)], axis=1))\n",
    "        \n",
    "    return tf.keras.losses.MSE(qvalues_target, q_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@tf.function\n",
    "def train_nn(samples, gamma, optimizer, q_new, q_nn):\n",
    "    \"\"\"\n",
    "    Updates the weights of the Q networks.\n",
    "    \n",
    "    Args:\n",
    "      experiences: (tuple) tuple of [\"state\", \"action\", \"reward\", \"next_state\", \"done\"] namedtuples\n",
    "      gamma: (float) The discount factor.\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    with tf.GradientTape() as tape:\n",
    "        loss =  compute_loss(samples, q_new, q_nn, gamma)\n",
    "\n",
    "    # Get the gradients of the loss with respect to the weights.\n",
    "    gradients = tape.gradient(loss, q_new.trainable_variables)\n",
    "    \n",
    "    # Update the weights of the q_network.\n",
    "    optimizer.apply_gradients(zip(gradients, q_new.trainable_variables))\n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-11 22:56:21.257758: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX512_VNNI\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-03-11 22:56:21.293315: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: \n"
     ]
    }
   ],
   "source": [
    "q_nn = create_NN()\n",
    "q_new = create_NN()\n",
    "\n",
    "optimizer=keras.optimizers.Adam(learning_rate=ALPHA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "env = gym.make('FrozenLake-v1', desc=None, map_name=\"8x8\", is_slippery=True, render_mode=\"human\")\n",
    "state, _ = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# buffer to store the tupples (state, action, reward, state_prime)\n",
    "buffer = deque(maxlen=UPDATE_STEP)\n",
    "\n",
    "# Namedtuple to hold the data points\n",
    "data_point = namedtuple(\"data_point\", [\"state\", \"action\", \"reward\", \"state_prime\", \"done\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "8\n",
      "9\n",
      "1\n",
      "2\n",
      "3\n",
      "3\n",
      "4\n",
      "12\n",
      "20\n",
      "12\n",
      "20\n",
      "21\n",
      "22\n",
      "14\n",
      "22\n",
      "30\n",
      "31\n",
      "31\n",
      "23\n",
      "[data_point(state=2, action=2, reward=0.0, state_prime=3, done=False), data_point(state=31, action=2, reward=0.0, state_prime=23, done=False), data_point(state=3, action=0, reward=0.0, state_prime=3, done=False), data_point(state=30, action=2, reward=0.0, state_prime=31, done=False), data_point(state=12, action=2, reward=0.0, state_prime=20, done=False), data_point(state=22, action=2, reward=0.0, state_prime=14, done=False), data_point(state=4, action=2, reward=0.0, state_prime=12, done=False), data_point(state=20, action=2, reward=0.0, state_prime=21, done=False), data_point(state=14, action=2, reward=0.0, state_prime=22, done=False), data_point(state=21, action=2, reward=0.0, state_prime=22, done=False), data_point(state=31, action=2, reward=0.0, state_prime=31, done=False), data_point(state=20, action=2, reward=0.0, state_prime=12, done=False), data_point(state=22, action=2, reward=0.0, state_prime=30, done=False), data_point(state=1, action=2, reward=0.0, state_prime=2, done=False), data_point(state=3, action=2, reward=0.0, state_prime=4, done=False), data_point(state=23, action=2, reward=0.0, state_prime=31, done=False), data_point(state=8, action=2, reward=0.0, state_prime=9, done=False), data_point(state=0, action=1, reward=0.0, state_prime=8, done=False), data_point(state=12, action=2, reward=0.0, state_prime=20, done=False), data_point(state=9, action=2, reward=0.0, state_prime=1, done=False)]\n",
      "31\n",
      "39\n",
      "47\n",
      "47\n",
      "55\n",
      "47\n",
      "39\n",
      "39\n",
      "47\n",
      "39\n",
      "47\n",
      "55\n",
      "55\n",
      "47\n",
      "55\n",
      "47\n",
      "47\n",
      "55\n",
      "0\n",
      "1\n",
      "[data_point(state=1, action=2, reward=0.0, state_prime=2, done=False), data_point(state=47, action=2, reward=0.0, state_prime=39, done=False), data_point(state=39, action=2, reward=0.0, state_prime=47, done=False), data_point(state=55, action=2, reward=0.0, state_prime=47, done=False), data_point(state=47, action=2, reward=0.0, state_prime=47, done=False), data_point(state=47, action=2, reward=0.0, state_prime=55, done=False), data_point(state=47, action=2, reward=0.0, state_prime=55, done=False), data_point(state=39, action=2, reward=0.0, state_prime=47, done=False), data_point(state=39, action=2, reward=0.0, state_prime=39, done=False), data_point(state=47, action=2, reward=0.0, state_prime=55, done=False), data_point(state=0, action=3, reward=0.0, state_prime=1, done=False), data_point(state=55, action=2, reward=0.0, state_prime=47, done=False), data_point(state=55, action=2, reward=0.0, state_prime=55, done=False), data_point(state=55, action=2, reward=1.0, state_prime=63, done=True), data_point(state=47, action=2, reward=0.0, state_prime=39, done=False), data_point(state=31, action=2, reward=0.0, state_prime=39, done=False), data_point(state=39, action=2, reward=0.0, state_prime=47, done=False), data_point(state=47, action=2, reward=0.0, state_prime=47, done=False), data_point(state=47, action=2, reward=0.0, state_prime=55, done=False), data_point(state=55, action=2, reward=0.0, state_prime=47, done=False)]\n",
      "2\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "11\n",
      "0\n",
      "1\n",
      "9\n",
      "17\n",
      "25\n",
      "33\n",
      "34\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "2\n",
      "10\n",
      "11\n",
      "[data_point(state=0, action=3, reward=0.0, state_prime=1, done=False), data_point(state=0, action=3, reward=0.0, state_prime=0, done=False), data_point(state=11, action=2, reward=0.0, state_prime=19, done=True), data_point(state=3, action=2, reward=0.0, state_prime=3, done=False), data_point(state=2, action=2, reward=0.0, state_prime=10, done=False), data_point(state=2, action=2, reward=0.0, state_prime=3, done=False), data_point(state=17, action=2, reward=0.0, state_prime=25, done=False), data_point(state=10, action=2, reward=0.0, state_prime=11, done=False), data_point(state=33, action=2, reward=0.0, state_prime=34, done=False), data_point(state=9, action=2, reward=0.0, state_prime=17, done=False), data_point(state=3, action=2, reward=0.0, state_prime=3, done=False), data_point(state=25, action=2, reward=0.0, state_prime=33, done=False), data_point(state=1, action=2, reward=0.0, state_prime=9, done=False), data_point(state=3, action=2, reward=0.0, state_prime=11, done=False), data_point(state=1, action=2, reward=0.0, state_prime=2, done=False), data_point(state=0, action=3, reward=0.0, state_prime=1, done=False), data_point(state=0, action=3, reward=0.0, state_prime=0, done=False), data_point(state=34, action=2, reward=0.0, state_prime=35, done=True), data_point(state=3, action=2, reward=0.0, state_prime=3, done=False), data_point(state=11, action=2, reward=0.0, state_prime=19, done=True)]\n",
      "0\n",
      "0\n",
      "1\n",
      "9\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "7\n",
      "15\n",
      "15\n",
      "15\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "[data_point(state=15, action=2, reward=0.0, state_prime=15, done=False), data_point(state=15, action=2, reward=0.0, state_prime=7, done=False), data_point(state=13, action=2, reward=0.0, state_prime=14, done=False), data_point(state=3, action=2, reward=0.0, state_prime=4, done=False), data_point(state=7, action=2, reward=0.0, state_prime=7, done=False), data_point(state=4, action=2, reward=0.0, state_prime=12, done=False), data_point(state=15, action=2, reward=0.0, state_prime=7, done=False), data_point(state=7, action=2, reward=0.0, state_prime=7, done=False), data_point(state=7, action=2, reward=0.0, state_prime=15, done=False), data_point(state=0, action=3, reward=0.0, state_prime=1, done=False), data_point(state=12, action=2, reward=0.0, state_prime=13, done=False), data_point(state=15, action=2, reward=0.0, state_prime=15, done=False), data_point(state=14, action=2, reward=0.0, state_prime=15, done=False), data_point(state=2, action=2, reward=0.0, state_prime=3, done=False), data_point(state=1, action=2, reward=0.0, state_prime=2, done=False), data_point(state=0, action=3, reward=0.0, state_prime=0, done=False), data_point(state=7, action=2, reward=0.0, state_prime=7, done=False), data_point(state=9, action=2, reward=0.0, state_prime=1, done=False), data_point(state=7, action=2, reward=0.0, state_prime=7, done=False), data_point(state=1, action=2, reward=0.0, state_prime=9, done=False)]\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "15\n",
      "7\n",
      "7\n",
      "7\n",
      "15\n",
      "7\n",
      "15\n",
      "23\n",
      "23\n",
      "31\n",
      "39\n",
      "31\n",
      "39\n",
      "47\n",
      "39\n",
      "[data_point(state=7, action=2, reward=0.0, state_prime=15, done=False), data_point(state=31, action=2, reward=0.0, state_prime=39, done=False), data_point(state=39, action=2, reward=0.0, state_prime=47, done=False), data_point(state=7, action=2, reward=0.0, state_prime=7, done=False), data_point(state=15, action=2, reward=0.0, state_prime=23, done=False), data_point(state=39, action=2, reward=0.0, state_prime=47, done=False), data_point(state=31, action=2, reward=0.0, state_prime=39, done=False), data_point(state=39, action=2, reward=0.0, state_prime=31, done=False), data_point(state=7, action=2, reward=0.0, state_prime=15, done=False), data_point(state=7, action=2, reward=0.0, state_prime=7, done=False), data_point(state=23, action=2, reward=0.0, state_prime=31, done=False), data_point(state=7, action=2, reward=0.0, state_prime=7, done=False), data_point(state=7, action=2, reward=0.0, state_prime=7, done=False), data_point(state=47, action=2, reward=0.0, state_prime=39, done=False), data_point(state=7, action=2, reward=0.0, state_prime=7, done=False), data_point(state=15, action=2, reward=0.0, state_prime=7, done=False), data_point(state=15, action=2, reward=0.0, state_prime=7, done=False), data_point(state=7, action=2, reward=0.0, state_prime=15, done=False), data_point(state=7, action=2, reward=0.0, state_prime=7, done=False), data_point(state=23, action=2, reward=0.0, state_prime=23, done=False)]\n",
      "WARNING:tensorflow:5 out of the last 5 calls to <function train_nn at 0x7f8b9315ddc0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "47\n",
      "55\n",
      "55\n",
      "0\n",
      "0\n",
      "1\n",
      "2\n",
      "10\n",
      "18\n",
      "0\n",
      "8\n",
      "9\n",
      "1\n",
      "9\n",
      "17\n",
      "18\n",
      "26\n",
      "34\n",
      "26\n",
      "34\n",
      "[data_point(state=18, action=2, reward=0.0, state_prime=19, done=True), data_point(state=2, action=2, reward=0.0, state_prime=10, done=False), data_point(state=26, action=2, reward=0.0, state_prime=34, done=False), data_point(state=18, action=2, reward=0.0, state_prime=26, done=False), data_point(state=0, action=2, reward=0.0, state_prime=8, done=False), data_point(state=1, action=2, reward=0.0, state_prime=2, done=False), data_point(state=10, action=2, reward=0.0, state_prime=18, done=False), data_point(state=47, action=2, reward=0.0, state_prime=55, done=False), data_point(state=8, action=2, reward=0.0, state_prime=9, done=False), data_point(state=34, action=2, reward=0.0, state_prime=26, done=False), data_point(state=17, action=2, reward=0.0, state_prime=18, done=False), data_point(state=55, action=2, reward=0.0, state_prime=55, done=False), data_point(state=9, action=2, reward=0.0, state_prime=17, done=False), data_point(state=55, action=2, reward=1.0, state_prime=63, done=True), data_point(state=0, action=2, reward=0.0, state_prime=1, done=False), data_point(state=26, action=2, reward=0.0, state_prime=34, done=False), data_point(state=9, action=2, reward=0.0, state_prime=1, done=False), data_point(state=34, action=2, reward=0.0, state_prime=26, done=False), data_point(state=1, action=2, reward=0.0, state_prime=9, done=False), data_point(state=0, action=2, reward=0.0, state_prime=0, done=False)]\n",
      "WARNING:tensorflow:6 out of the last 6 calls to <function train_nn at 0x7f8b9315ddc0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "26\n",
      "34\n",
      "0\n",
      "0\n",
      "8\n",
      "0\n",
      "0\n",
      "8\n",
      "0\n",
      "8\n",
      "0\n",
      "1\n",
      "9\n",
      "17\n",
      "9\n",
      "10\n",
      "18\n",
      "0\n",
      "8\n",
      "0\n",
      "[data_point(state=34, action=2, reward=0.0, state_prime=35, done=True), data_point(state=8, action=2, reward=0.0, state_prime=0, done=False), data_point(state=1, action=2, reward=0.0, state_prime=9, done=False), data_point(state=0, action=2, reward=0.0, state_prime=1, done=False), data_point(state=10, action=2, reward=0.0, state_prime=18, done=False), data_point(state=8, action=2, reward=0.0, state_prime=0, done=False), data_point(state=0, action=2, reward=0.0, state_prime=0, done=False), data_point(state=8, action=2, reward=0.0, state_prime=0, done=False), data_point(state=18, action=2, reward=0.0, state_prime=19, done=True), data_point(state=9, action=2, reward=0.0, state_prime=10, done=False), data_point(state=17, action=2, reward=0.0, state_prime=9, done=False), data_point(state=0, action=2, reward=0.0, state_prime=8, done=False), data_point(state=0, action=2, reward=0.0, state_prime=8, done=False), data_point(state=0, action=2, reward=0.0, state_prime=8, done=False), data_point(state=9, action=2, reward=0.0, state_prime=17, done=False), data_point(state=8, action=2, reward=0.0, state_prime=0, done=False), data_point(state=0, action=2, reward=0.0, state_prime=8, done=False), data_point(state=0, action=2, reward=0.0, state_prime=8, done=False), data_point(state=0, action=2, reward=0.0, state_prime=0, done=False), data_point(state=26, action=2, reward=0.0, state_prime=34, done=False)]\n",
      "8\n",
      "16\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "0\n",
      "0\n",
      "8\n",
      "9\n",
      "17\n",
      "9\n",
      "1\n",
      "1\n",
      "9\n",
      "10\n",
      "2\n",
      "10\n",
      "18\n",
      "26\n",
      "[data_point(state=0, action=2, reward=0.0, state_prime=0, done=False), data_point(state=9, action=2, reward=0.0, state_prime=17, done=False), data_point(state=26, action=2, reward=0.0, state_prime=27, done=False), data_point(state=27, action=2, reward=0.0, state_prime=35, done=True), data_point(state=16, action=2, reward=0.0, state_prime=24, done=False), data_point(state=1, action=2, reward=0.0, state_prime=9, done=False), data_point(state=9, action=2, reward=0.0, state_prime=1, done=False), data_point(state=8, action=2, reward=0.0, state_prime=9, done=False), data_point(state=2, action=2, reward=0.0, state_prime=10, done=False), data_point(state=10, action=2, reward=0.0, state_prime=18, done=False), data_point(state=8, action=2, reward=0.0, state_prime=16, done=False), data_point(state=26, action=2, reward=0.0, state_prime=27, done=False), data_point(state=25, action=2, reward=0.0, state_prime=26, done=False), data_point(state=24, action=2, reward=0.0, state_prime=25, done=False), data_point(state=9, action=2, reward=0.0, state_prime=10, done=False), data_point(state=1, action=2, reward=0.0, state_prime=1, done=False), data_point(state=0, action=2, reward=0.0, state_prime=8, done=False), data_point(state=17, action=2, reward=0.0, state_prime=9, done=False), data_point(state=10, action=2, reward=0.0, state_prime=2, done=False), data_point(state=18, action=2, reward=0.0, state_prime=26, done=False)]\n",
      "27\n",
      "0\n",
      "1\n",
      "1\n",
      "2\n",
      "3\n",
      "3\n",
      "11\n",
      "0\n",
      "1\n",
      "1\n",
      "9\n",
      "10\n",
      "18\n",
      "26\n",
      "27\n",
      "28\n",
      "36\n",
      "28\n",
      "0\n",
      "[data_point(state=27, action=2, reward=0.0, state_prime=35, done=True), data_point(state=28, action=2, reward=0.0, state_prime=36, done=False), data_point(state=27, action=2, reward=0.0, state_prime=28, done=False), data_point(state=28, action=2, reward=0.0, state_prime=29, done=True), data_point(state=1, action=2, reward=0.0, state_prime=2, done=False), data_point(state=36, action=2, reward=0.0, state_prime=28, done=False), data_point(state=0, action=2, reward=0.0, state_prime=1, done=False), data_point(state=1, action=2, reward=0.0, state_prime=1, done=False), data_point(state=1, action=2, reward=0.0, state_prime=9, done=False), data_point(state=26, action=2, reward=0.0, state_prime=27, done=False), data_point(state=10, action=2, reward=0.0, state_prime=18, done=False), data_point(state=1, action=2, reward=0.0, state_prime=1, done=False), data_point(state=18, action=2, reward=0.0, state_prime=26, done=False), data_point(state=0, action=2, reward=0.0, state_prime=1, done=False), data_point(state=3, action=2, reward=0.0, state_prime=3, done=False), data_point(state=9, action=2, reward=0.0, state_prime=10, done=False), data_point(state=2, action=2, reward=0.0, state_prime=3, done=False), data_point(state=0, action=2, reward=0.0, state_prime=1, done=False), data_point(state=11, action=2, reward=0.0, state_prime=19, done=True), data_point(state=3, action=2, reward=0.0, state_prime=11, done=False)]\n",
      "1\n",
      "9\n",
      "10\n",
      "2\n",
      "10\n",
      "11\n",
      "12\n",
      "4\n",
      "4\n",
      "5\n",
      "13\n",
      "14\n",
      "22\n",
      "23\n",
      "31\n",
      "23\n",
      "23\n",
      "15\n",
      "7\n",
      "7\n",
      "[data_point(state=11, action=2, reward=0.0, state_prime=12, done=False), data_point(state=9, action=2, reward=0.0, state_prime=10, done=False), data_point(state=4, action=2, reward=0.0, state_prime=4, done=False), data_point(state=22, action=2, reward=0.0, state_prime=23, done=False), data_point(state=1, action=2, reward=0.0, state_prime=9, done=False), data_point(state=4, action=2, reward=0.0, state_prime=5, done=False), data_point(state=5, action=2, reward=0.0, state_prime=13, done=False), data_point(state=13, action=2, reward=0.0, state_prime=14, done=False), data_point(state=10, action=2, reward=0.0, state_prime=2, done=False), data_point(state=2, action=2, reward=0.0, state_prime=10, done=False), data_point(state=31, action=2, reward=0.0, state_prime=23, done=False), data_point(state=23, action=2, reward=0.0, state_prime=15, done=False), data_point(state=14, action=2, reward=0.0, state_prime=22, done=False), data_point(state=12, action=2, reward=0.0, state_prime=4, done=False), data_point(state=23, action=2, reward=0.0, state_prime=31, done=False), data_point(state=15, action=2, reward=0.0, state_prime=7, done=False), data_point(state=7, action=2, reward=0.0, state_prime=7, done=False), data_point(state=23, action=2, reward=0.0, state_prime=23, done=False), data_point(state=7, action=2, reward=0.0, state_prime=7, done=False), data_point(state=10, action=2, reward=0.0, state_prime=11, done=False)]\n",
      "7\n",
      "15\n",
      "15\n",
      "15\n",
      "15\n",
      "23\n",
      "15\n",
      "15\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "15\n",
      "23\n",
      "31\n",
      "39\n",
      "47\n",
      "39\n",
      "[data_point(state=15, action=2, reward=0.0, state_prime=15, done=False), data_point(state=15, action=2, reward=0.0, state_prime=23, done=False), data_point(state=7, action=2, reward=0.0, state_prime=7, done=False), data_point(state=7, action=2, reward=0.0, state_prime=15, done=False), data_point(state=15, action=2, reward=0.0, state_prime=23, done=False), data_point(state=7, action=2, reward=0.0, state_prime=7, done=False), data_point(state=39, action=2, reward=0.0, state_prime=47, done=False), data_point(state=15, action=2, reward=0.0, state_prime=15, done=False), data_point(state=15, action=2, reward=0.0, state_prime=15, done=False), data_point(state=31, action=2, reward=0.0, state_prime=39, done=False), data_point(state=23, action=2, reward=0.0, state_prime=31, done=False), data_point(state=15, action=2, reward=0.0, state_prime=15, done=False), data_point(state=23, action=2, reward=0.0, state_prime=15, done=False), data_point(state=7, action=2, reward=0.0, state_prime=7, done=False), data_point(state=7, action=2, reward=0.0, state_prime=7, done=False), data_point(state=7, action=2, reward=0.0, state_prime=7, done=False), data_point(state=39, action=2, reward=0.0, state_prime=39, done=False), data_point(state=7, action=2, reward=0.0, state_prime=15, done=False), data_point(state=47, action=2, reward=0.0, state_prime=39, done=False), data_point(state=15, action=2, reward=0.0, state_prime=7, done=False)]\n",
      "39\n",
      "47\n",
      "55\n",
      "47\n",
      "39\n",
      "39\n",
      "39\n",
      "39\n",
      "47\n",
      "39\n",
      "31\n",
      "23\n",
      "31\n",
      "31\n",
      "39\n",
      "47\n",
      "47\n",
      "47\n",
      "47\n",
      "47\n",
      "[data_point(state=47, action=2, reward=0.0, state_prime=55, done=False), data_point(state=39, action=2, reward=0.0, state_prime=47, done=False), data_point(state=47, action=2, reward=0.0, state_prime=39, done=False), data_point(state=39, action=2, reward=0.0, state_prime=31, done=False), data_point(state=47, action=2, reward=0.0, state_prime=55, done=False), data_point(state=39, action=2, reward=0.0, state_prime=47, done=False), data_point(state=31, action=2, reward=0.0, state_prime=31, done=False), data_point(state=47, action=2, reward=0.0, state_prime=47, done=False), data_point(state=47, action=2, reward=0.0, state_prime=47, done=False), data_point(state=31, action=2, reward=0.0, state_prime=39, done=False), data_point(state=39, action=2, reward=0.0, state_prime=47, done=False), data_point(state=47, action=2, reward=0.0, state_prime=47, done=False), data_point(state=31, action=2, reward=0.0, state_prime=23, done=False), data_point(state=23, action=2, reward=0.0, state_prime=31, done=False), data_point(state=47, action=2, reward=0.0, state_prime=39, done=False), data_point(state=47, action=2, reward=0.0, state_prime=47, done=False), data_point(state=39, action=2, reward=0.0, state_prime=39, done=False), data_point(state=39, action=2, reward=0.0, state_prime=39, done=False), data_point(state=55, action=2, reward=0.0, state_prime=47, done=False), data_point(state=39, action=2, reward=0.0, state_prime=39, done=False)]\n",
      "55\n",
      "0\n",
      "8\n",
      "0\n",
      "8\n",
      "16\n",
      "17\n",
      "18\n",
      "26\n",
      "27\n",
      "0\n",
      "1\n",
      "1\n",
      "2\n",
      "10\n",
      "18\n",
      "10\n",
      "18\n",
      "26\n",
      "34\n",
      "[data_point(state=2, action=2, reward=0.0, state_prime=10, done=False), data_point(state=26, action=2, reward=0.0, state_prime=34, done=False), data_point(state=18, action=2, reward=0.0, state_prime=26, done=False), data_point(state=16, action=2, reward=0.0, state_prime=17, done=False), data_point(state=10, action=2, reward=0.0, state_prime=18, done=False), data_point(state=10, action=2, reward=0.0, state_prime=18, done=False), data_point(state=26, action=2, reward=0.0, state_prime=27, done=False), data_point(state=0, action=2, reward=0.0, state_prime=8, done=False), data_point(state=27, action=2, reward=0.0, state_prime=35, done=True), data_point(state=1, action=2, reward=0.0, state_prime=2, done=False), data_point(state=0, action=2, reward=0.0, state_prime=1, done=False), data_point(state=8, action=2, reward=0.0, state_prime=16, done=False), data_point(state=8, action=2, reward=0.0, state_prime=0, done=False), data_point(state=34, action=2, reward=0.0, state_prime=35, done=True), data_point(state=1, action=2, reward=0.0, state_prime=1, done=False), data_point(state=18, action=2, reward=0.0, state_prime=26, done=False), data_point(state=18, action=2, reward=0.0, state_prime=10, done=False), data_point(state=55, action=2, reward=1.0, state_prime=63, done=True), data_point(state=0, action=2, reward=0.0, state_prime=8, done=False), data_point(state=17, action=2, reward=0.0, state_prime=18, done=False)]\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "8\n",
      "9\n",
      "1\n",
      "1\n",
      "9\n",
      "1\n",
      "9\n",
      "17\n",
      "18\n",
      "10\n",
      "2\n",
      "10\n",
      "2\n",
      "[data_point(state=2, action=2, reward=0.0, state_prime=10, done=False), data_point(state=0, action=2, reward=0.0, state_prime=8, done=False), data_point(state=10, action=2, reward=0.0, state_prime=2, done=False), data_point(state=1, action=2, reward=0.0, state_prime=9, done=False), data_point(state=9, action=2, reward=0.0, state_prime=1, done=False), data_point(state=8, action=2, reward=0.0, state_prime=9, done=False), data_point(state=1, action=2, reward=0.0, state_prime=1, done=False), data_point(state=9, action=2, reward=0.0, state_prime=1, done=False), data_point(state=9, action=2, reward=0.0, state_prime=17, done=False), data_point(state=0, action=2, reward=0.0, state_prime=0, done=False), data_point(state=2, action=2, reward=0.0, state_prime=10, done=False), data_point(state=0, action=2, reward=0.0, state_prime=0, done=False), data_point(state=0, action=2, reward=0.0, state_prime=0, done=False), data_point(state=0, action=2, reward=0.0, state_prime=0, done=False), data_point(state=18, action=2, reward=0.0, state_prime=10, done=False), data_point(state=1, action=2, reward=0.0, state_prime=9, done=False), data_point(state=17, action=2, reward=0.0, state_prime=18, done=False), data_point(state=0, action=2, reward=0.0, state_prime=0, done=False), data_point(state=0, action=2, reward=0.0, state_prime=0, done=False), data_point(state=10, action=0, reward=0.0, state_prime=2, done=False)]\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "21\n",
      "22\n",
      "23\n",
      "23\n",
      "23\n",
      "15\n",
      "23\n",
      "23\n",
      "31\n",
      "31\n",
      "31\n",
      "31\n",
      "23\n",
      "23\n",
      "31\n",
      "39\n",
      "[data_point(state=31, action=2, reward=0.0, state_prime=31, done=False), data_point(state=23, action=2, reward=0.0, state_prime=31, done=False), data_point(state=23, action=2, reward=0.0, state_prime=23, done=False), data_point(state=31, action=2, reward=0.0, state_prime=31, done=False), data_point(state=12, action=2, reward=0.0, state_prime=13, done=False), data_point(state=23, action=2, reward=0.0, state_prime=23, done=False), data_point(state=31, action=2, reward=0.0, state_prime=31, done=False), data_point(state=23, action=2, reward=0.0, state_prime=31, done=False), data_point(state=31, action=2, reward=0.0, state_prime=23, done=False), data_point(state=39, action=2, reward=0.0, state_prime=47, done=False), data_point(state=22, action=2, reward=0.0, state_prime=23, done=False), data_point(state=11, action=2, reward=0.0, state_prime=12, done=False), data_point(state=31, action=2, reward=0.0, state_prime=39, done=False), data_point(state=13, action=2, reward=0.0, state_prime=21, done=False), data_point(state=23, action=2, reward=0.0, state_prime=15, done=False), data_point(state=15, action=2, reward=0.0, state_prime=23, done=False), data_point(state=23, action=2, reward=0.0, state_prime=23, done=False), data_point(state=10, action=2, reward=0.0, state_prime=11, done=False), data_point(state=21, action=2, reward=0.0, state_prime=22, done=False), data_point(state=23, action=2, reward=0.0, state_prime=23, done=False)]\n",
      "47\n",
      "47\n",
      "55\n",
      "47\n",
      "39\n",
      "39\n",
      "39\n",
      "39\n",
      "47\n",
      "55\n",
      "55\n",
      "47\n",
      "39\n",
      "39\n",
      "47\n",
      "47\n",
      "39\n",
      "39\n",
      "31\n",
      "23\n",
      "[data_point(state=39, action=2, reward=0.0, state_prime=39, done=False), data_point(state=55, action=2, reward=0.0, state_prime=47, done=False), data_point(state=39, action=2, reward=0.0, state_prime=39, done=False), data_point(state=39, action=2, reward=0.0, state_prime=47, done=False), data_point(state=39, action=2, reward=0.0, state_prime=31, done=False), data_point(state=47, action=2, reward=0.0, state_prime=47, done=False), data_point(state=47, action=2, reward=0.0, state_prime=47, done=False), data_point(state=47, action=2, reward=0.0, state_prime=39, done=False), data_point(state=47, action=2, reward=0.0, state_prime=55, done=False), data_point(state=47, action=2, reward=0.0, state_prime=39, done=False), data_point(state=39, action=2, reward=0.0, state_prime=39, done=False), data_point(state=31, action=2, reward=0.0, state_prime=23, done=False), data_point(state=23, action=2, reward=0.0, state_prime=31, done=False), data_point(state=55, action=2, reward=0.0, state_prime=55, done=False), data_point(state=39, action=2, reward=0.0, state_prime=47, done=False), data_point(state=39, action=2, reward=0.0, state_prime=39, done=False), data_point(state=47, action=2, reward=0.0, state_prime=39, done=False), data_point(state=39, action=2, reward=0.0, state_prime=39, done=False), data_point(state=47, action=2, reward=0.0, state_prime=55, done=False), data_point(state=55, action=2, reward=0.0, state_prime=47, done=False)]\n",
      "31\n",
      "31\n",
      "39\n",
      "31\n",
      "39\n",
      "31\n",
      "23\n",
      "23\n",
      "31\n",
      "39\n",
      "39\n",
      "39\n",
      "47\n",
      "55\n",
      "0\n",
      "1\n",
      "2\n",
      "10\n",
      "2\n",
      "2\n",
      "[data_point(state=2, action=2, reward=0.0, state_prime=2, done=False), data_point(state=23, action=2, reward=0.0, state_prime=23, done=False), data_point(state=1, action=2, reward=0.0, state_prime=2, done=False), data_point(state=31, action=2, reward=0.0, state_prime=39, done=False), data_point(state=31, action=2, reward=0.0, state_prime=39, done=False), data_point(state=31, action=2, reward=0.0, state_prime=23, done=False), data_point(state=39, action=2, reward=0.0, state_prime=47, done=False), data_point(state=23, action=2, reward=0.0, state_prime=31, done=False), data_point(state=2, action=2, reward=0.0, state_prime=3, done=False), data_point(state=47, action=2, reward=0.0, state_prime=55, done=False), data_point(state=31, action=2, reward=0.0, state_prime=31, done=False), data_point(state=55, action=2, reward=1.0, state_prime=63, done=True), data_point(state=10, action=2, reward=0.0, state_prime=2, done=False), data_point(state=39, action=2, reward=0.0, state_prime=39, done=False), data_point(state=0, action=2, reward=0.0, state_prime=1, done=False), data_point(state=31, action=0, reward=0.0, state_prime=39, done=False), data_point(state=2, action=2, reward=0.0, state_prime=10, done=False), data_point(state=39, action=2, reward=0.0, state_prime=39, done=False), data_point(state=39, action=2, reward=0.0, state_prime=31, done=False), data_point(state=39, action=2, reward=0.0, state_prime=31, done=False)]\n",
      "3\n",
      "3\n",
      "11\n",
      "12\n",
      "4\n",
      "5\n",
      "6\n",
      "6\n",
      "14\n",
      "6\n",
      "14\n",
      "6\n",
      "7\n",
      "15\n",
      "23\n",
      "31\n",
      "31\n",
      "31\n",
      "39\n",
      "39\n",
      "[data_point(state=31, action=2, reward=0.0, state_prime=39, done=False), data_point(state=3, action=2, reward=0.0, state_prime=11, done=False), data_point(state=7, action=2, reward=0.0, state_prime=15, done=False), data_point(state=3, action=2, reward=0.0, state_prime=3, done=False), data_point(state=5, action=2, reward=0.0, state_prime=6, done=False), data_point(state=14, action=2, reward=0.0, state_prime=6, done=False), data_point(state=14, action=2, reward=0.0, state_prime=6, done=False), data_point(state=6, action=2, reward=0.0, state_prime=7, done=False), data_point(state=39, action=2, reward=0.0, state_prime=39, done=False), data_point(state=12, action=2, reward=0.0, state_prime=4, done=False), data_point(state=15, action=2, reward=0.0, state_prime=23, done=False), data_point(state=23, action=2, reward=0.0, state_prime=31, done=False), data_point(state=6, action=2, reward=0.0, state_prime=14, done=False), data_point(state=11, action=2, reward=0.0, state_prime=12, done=False), data_point(state=31, action=2, reward=0.0, state_prime=31, done=False), data_point(state=6, action=2, reward=0.0, state_prime=14, done=False), data_point(state=39, action=2, reward=0.0, state_prime=39, done=False), data_point(state=6, action=2, reward=0.0, state_prime=6, done=False), data_point(state=31, action=2, reward=0.0, state_prime=31, done=False), data_point(state=4, action=2, reward=0.0, state_prime=5, done=False)]\n",
      "39\n",
      "31\n",
      "23\n",
      "23\n",
      "22\n",
      "30\n",
      "38\n",
      "0\n",
      "1\n",
      "2\n",
      "2\n",
      "3\n",
      "4\n",
      "12\n",
      "4\n",
      "5\n",
      "6\n",
      "14\n",
      "22\n",
      "30\n",
      "[data_point(state=14, action=2, reward=0.0, state_prime=22, done=False), data_point(state=4, action=2, reward=0.0, state_prime=12, done=False), data_point(state=30, action=2, reward=0.0, state_prime=31, done=False), data_point(state=2, action=2, reward=0.0, state_prime=2, done=False), data_point(state=0, action=3, reward=0.0, state_prime=1, done=False), data_point(state=4, action=2, reward=0.0, state_prime=5, done=False), data_point(state=3, action=2, reward=0.0, state_prime=4, done=False), data_point(state=23, action=3, reward=0.0, state_prime=22, done=False), data_point(state=1, action=2, reward=0.0, state_prime=2, done=False), data_point(state=5, action=2, reward=0.0, state_prime=6, done=False), data_point(state=22, action=2, reward=0.0, state_prime=30, done=False), data_point(state=38, action=2, reward=0.0, state_prime=46, done=True), data_point(state=39, action=2, reward=0.0, state_prime=31, done=False), data_point(state=30, action=2, reward=0.0, state_prime=38, done=False), data_point(state=2, action=2, reward=0.0, state_prime=3, done=False), data_point(state=6, action=2, reward=0.0, state_prime=14, done=False), data_point(state=23, action=2, reward=0.0, state_prime=23, done=False), data_point(state=22, action=2, reward=0.0, state_prime=30, done=False), data_point(state=31, action=2, reward=0.0, state_prime=23, done=False), data_point(state=12, action=2, reward=0.0, state_prime=4, done=False)]\n",
      "31\n",
      "39\n",
      "39\n",
      "39\n",
      "39\n",
      "39\n",
      "39\n",
      "31\n",
      "23\n",
      "15\n",
      "23\n",
      "23\n",
      "23\n",
      "23\n",
      "23\n",
      "31\n",
      "23\n",
      "15\n",
      "23\n",
      "31\n",
      "[data_point(state=39, action=2, reward=0.0, state_prime=39, done=False), data_point(state=15, action=2, reward=0.0, state_prime=23, done=False), data_point(state=31, action=2, reward=0.0, state_prime=39, done=False), data_point(state=23, action=2, reward=0.0, state_prime=15, done=False), data_point(state=31, action=2, reward=0.0, state_prime=39, done=False), data_point(state=23, action=2, reward=0.0, state_prime=31, done=False), data_point(state=39, action=2, reward=0.0, state_prime=39, done=False), data_point(state=31, action=2, reward=0.0, state_prime=23, done=False), data_point(state=23, action=2, reward=0.0, state_prime=23, done=False), data_point(state=31, action=2, reward=0.0, state_prime=23, done=False), data_point(state=39, action=2, reward=0.0, state_prime=39, done=False), data_point(state=39, action=2, reward=0.0, state_prime=31, done=False), data_point(state=23, action=2, reward=0.0, state_prime=31, done=False), data_point(state=39, action=2, reward=0.0, state_prime=39, done=False), data_point(state=15, action=2, reward=0.0, state_prime=23, done=False), data_point(state=39, action=2, reward=0.0, state_prime=39, done=False), data_point(state=23, action=2, reward=0.0, state_prime=23, done=False), data_point(state=23, action=2, reward=0.0, state_prime=23, done=False), data_point(state=23, action=2, reward=0.0, state_prime=15, done=False), data_point(state=23, action=2, reward=0.0, state_prime=23, done=False)]\n",
      "39\n",
      "47\n",
      "47\n",
      "47\n",
      "47\n",
      "47\n",
      "39\n",
      "39\n",
      "31\n",
      "31\n",
      "39\n",
      "39\n",
      "47\n",
      "47\n",
      "47\n",
      "39\n",
      "47\n",
      "39\n",
      "47\n",
      "47\n",
      "[data_point(state=31, action=2, reward=0.0, state_prime=39, done=False), data_point(state=39, action=2, reward=0.0, state_prime=31, done=False), data_point(state=47, action=2, reward=0.0, state_prime=47, done=False), data_point(state=39, action=2, reward=0.0, state_prime=47, done=False), data_point(state=47, action=2, reward=0.0, state_prime=39, done=False), data_point(state=47, action=2, reward=0.0, state_prime=47, done=False), data_point(state=39, action=2, reward=0.0, state_prime=47, done=False), data_point(state=47, action=2, reward=0.0, state_prime=47, done=False), data_point(state=47, action=2, reward=0.0, state_prime=47, done=False), data_point(state=47, action=2, reward=0.0, state_prime=39, done=False), data_point(state=47, action=2, reward=0.0, state_prime=47, done=False), data_point(state=39, action=2, reward=0.0, state_prime=47, done=False), data_point(state=39, action=2, reward=0.0, state_prime=47, done=False), data_point(state=39, action=2, reward=0.0, state_prime=39, done=False), data_point(state=47, action=2, reward=0.0, state_prime=39, done=False), data_point(state=47, action=2, reward=0.0, state_prime=39, done=False), data_point(state=39, action=2, reward=0.0, state_prime=39, done=False), data_point(state=47, action=2, reward=0.0, state_prime=47, done=False), data_point(state=47, action=2, reward=0.0, state_prime=47, done=False), data_point(state=31, action=2, reward=0.0, state_prime=31, done=False)]\n",
      "39\n",
      "47\n",
      "55\n",
      "55\n",
      "47\n",
      "55\n",
      "0\n",
      "0\n",
      "1\n",
      "2\n",
      "10\n",
      "11\n",
      "3\n",
      "4\n",
      "4\n",
      "5\n",
      "13\n",
      "5\n",
      "6\n",
      "14\n",
      "[data_point(state=55, action=2, reward=0.0, state_prime=47, done=False), data_point(state=6, action=2, reward=0.0, state_prime=14, done=False), data_point(state=47, action=2, reward=0.0, state_prime=55, done=False), data_point(state=47, action=2, reward=0.0, state_prime=55, done=False), data_point(state=0, action=3, reward=0.0, state_prime=0, done=False), data_point(state=0, action=3, reward=0.0, state_prime=1, done=False), data_point(state=14, action=2, reward=0.0, state_prime=6, done=False), data_point(state=10, action=2, reward=0.0, state_prime=11, done=False), data_point(state=4, action=2, reward=0.0, state_prime=5, done=False), data_point(state=39, action=2, reward=0.0, state_prime=47, done=False), data_point(state=1, action=2, reward=0.0, state_prime=2, done=False), data_point(state=5, action=2, reward=0.0, state_prime=6, done=False), data_point(state=13, action=2, reward=0.0, state_prime=5, done=False), data_point(state=55, action=2, reward=0.0, state_prime=55, done=False), data_point(state=3, action=2, reward=0.0, state_prime=4, done=False), data_point(state=4, action=2, reward=0.0, state_prime=4, done=False), data_point(state=5, action=2, reward=0.0, state_prime=13, done=False), data_point(state=11, action=2, reward=0.0, state_prime=3, done=False), data_point(state=55, action=2, reward=1.0, state_prime=63, done=True), data_point(state=2, action=2, reward=0.0, state_prime=10, done=False)]\n",
      "6\n",
      "6\n",
      "7\n",
      "7\n",
      "15\n",
      "7\n",
      "7\n",
      "15\n",
      "23\n",
      "23\n",
      "23\n",
      "15\n",
      "23\n",
      "31\n",
      "39\n",
      "31\n",
      "31\n",
      "31\n",
      "31\n",
      "30\n",
      "[data_point(state=31, action=3, reward=0.0, state_prime=30, done=False), data_point(state=39, action=2, reward=0.0, state_prime=31, done=False), data_point(state=6, action=2, reward=0.0, state_prime=6, done=False), data_point(state=7, action=2, reward=0.0, state_prime=7, done=False), data_point(state=31, action=2, reward=0.0, state_prime=31, done=False), data_point(state=15, action=2, reward=0.0, state_prime=7, done=False), data_point(state=7, action=2, reward=0.0, state_prime=7, done=False), data_point(state=6, action=2, reward=0.0, state_prime=7, done=False), data_point(state=23, action=2, reward=0.0, state_prime=23, done=False), data_point(state=31, action=2, reward=0.0, state_prime=31, done=False), data_point(state=7, action=2, reward=0.0, state_prime=15, done=False), data_point(state=23, action=2, reward=0.0, state_prime=15, done=False), data_point(state=31, action=2, reward=0.0, state_prime=39, done=False), data_point(state=15, action=2, reward=0.0, state_prime=23, done=False), data_point(state=7, action=2, reward=0.0, state_prime=15, done=False), data_point(state=15, action=2, reward=0.0, state_prime=23, done=False), data_point(state=23, action=2, reward=0.0, state_prime=23, done=False), data_point(state=23, action=2, reward=0.0, state_prime=31, done=False), data_point(state=31, action=3, reward=0.0, state_prime=31, done=False), data_point(state=30, action=2, reward=0.0, state_prime=31, done=False)]\n",
      "31\n",
      "31\n",
      "23\n",
      "15\n",
      "15\n",
      "15\n",
      "23\n",
      "31\n",
      "31\n",
      "31\n",
      "31\n",
      "31\n",
      "23\n",
      "31\n",
      "31\n",
      "23\n",
      "31\n",
      "23\n",
      "23\n",
      "15\n",
      "[data_point(state=31, action=2, reward=0.0, state_prime=31, done=False), data_point(state=15, action=2, reward=0.0, state_prime=23, done=False), data_point(state=31, action=2, reward=0.0, state_prime=23, done=False), data_point(state=23, action=2, reward=0.0, state_prime=15, done=False), data_point(state=23, action=2, reward=0.0, state_prime=31, done=False), data_point(state=23, action=2, reward=0.0, state_prime=31, done=False), data_point(state=15, action=2, reward=0.0, state_prime=15, done=False), data_point(state=31, action=2, reward=0.0, state_prime=23, done=False), data_point(state=23, action=2, reward=0.0, state_prime=15, done=False), data_point(state=23, action=2, reward=0.0, state_prime=31, done=False), data_point(state=23, action=2, reward=0.0, state_prime=23, done=False), data_point(state=31, action=2, reward=0.0, state_prime=31, done=False), data_point(state=31, action=2, reward=0.0, state_prime=31, done=False), data_point(state=15, action=2, reward=0.0, state_prime=15, done=False), data_point(state=31, action=2, reward=0.0, state_prime=23, done=False), data_point(state=31, action=2, reward=0.0, state_prime=31, done=False), data_point(state=31, action=2, reward=0.0, state_prime=31, done=False), data_point(state=15, action=2, reward=0.0, state_prime=15, done=False), data_point(state=31, action=2, reward=0.0, state_prime=23, done=False), data_point(state=31, action=2, reward=0.0, state_prime=31, done=False)]\n",
      "15\n",
      "23\n",
      "23\n",
      "23\n",
      "31\n",
      "39\n",
      "31\n",
      "23\n",
      "15\n",
      "15\n",
      "15\n",
      "15\n",
      "23\n",
      "15\n",
      "7\n",
      "15\n",
      "15\n",
      "23\n",
      "23\n",
      "31\n",
      "[data_point(state=15, action=2, reward=0.0, state_prime=15, done=False), data_point(state=15, action=2, reward=0.0, state_prime=23, done=False), data_point(state=23, action=2, reward=0.0, state_prime=23, done=False), data_point(state=31, action=2, reward=0.0, state_prime=39, done=False), data_point(state=23, action=2, reward=0.0, state_prime=23, done=False), data_point(state=31, action=2, reward=0.0, state_prime=23, done=False), data_point(state=15, action=2, reward=0.0, state_prime=15, done=False), data_point(state=15, action=2, reward=0.0, state_prime=23, done=False), data_point(state=23, action=2, reward=0.0, state_prime=15, done=False), data_point(state=31, action=2, reward=0.0, state_prime=39, done=False), data_point(state=23, action=2, reward=0.0, state_prime=31, done=False), data_point(state=23, action=2, reward=0.0, state_prime=15, done=False), data_point(state=7, action=2, reward=0.0, state_prime=15, done=False), data_point(state=15, action=2, reward=0.0, state_prime=23, done=False), data_point(state=23, action=2, reward=0.0, state_prime=31, done=False), data_point(state=15, action=2, reward=0.0, state_prime=15, done=False), data_point(state=39, action=2, reward=0.0, state_prime=31, done=False), data_point(state=15, action=2, reward=0.0, state_prime=7, done=False), data_point(state=15, action=2, reward=0.0, state_prime=15, done=False), data_point(state=23, action=2, reward=0.0, state_prime=23, done=False)]\n"
     ]
    }
   ],
   "source": [
    "for i in range(ITERATIONS):\n",
    "    #print(state)\n",
    "    # Transform dimensions of state for qnetwork\n",
    "    state_qn = np.expand_dims(state, axis=0)\n",
    "    action = egreedy_select(state, q_nn, EPSILON)\n",
    "    state_prime, reward, done, _, _ = env.step(action)\n",
    "\n",
    "    env.render()\n",
    "    buffer.append(data_point(state, action, reward, state_prime, done))\n",
    "    if done:\n",
    "        # Go back to initial state\n",
    "        state, _ = env.reset()\n",
    "    else:\n",
    "        state = state_prime\n",
    "\n",
    "    if ((i+1) % UPDATE_STEP) == 0:\n",
    "        samples = random.sample(buffer,k=TRAIN_SAMPLES)\n",
    "        #print(samples)\n",
    "        \n",
    "        # Train the neural network\n",
    "        train_nn(samples, GAMMA, optimizer, q_new, q_nn)\n",
    "\n",
    "        # update the weights of target q_network\n",
    "        for q_nn_weights, q_new_weights in zip(\n",
    "            q_nn.weights, q_new.weights\n",
    "        ):\n",
    "            q_nn_weights.assign(TAU * q_new_weights + (1.0 - TAU) * q_nn_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "test_egreedy_select1 (__main__.TestNotebook)\n",
      "Test selection of the state with highest state action value ... ok\n",
      "test_egreedy_select2 (__main__.TestNotebook)\n",
      "Test random selection of the state when all state action values are the same ... ok\n",
      "test_egreedy_select3 (__main__.TestNotebook)\n",
      "Test random selection with epsilon <> 0 ... ok\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 3 tests in 0.295s\n",
      "\n",
      "OK\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<unittest.main.TestProgram at 0x7f8b36360f10>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Unit tests\n",
    "\n",
    "import unittest \n",
    "\n",
    "class TestNotebook(unittest.TestCase):\n",
    "\n",
    "    def test_egreedy_select1(self):\n",
    "        '''\n",
    "        Test selection of the state with highest state action value\n",
    "        '''\n",
    "        nn = tf.keras.models.Sequential([\n",
    "            tf.keras.layers.Input(shape=STATE_FEATURES),\n",
    "            tf.keras.layers.Dense(len(ACTIONS))\n",
    "        ])\n",
    "\n",
    "        nn.compile(\n",
    "            optimizer=keras.optimizers.Adam(learning_rate=0.01),\n",
    "            loss=tf.keras.losses.MeanSquaredError()\n",
    "        )\n",
    "\n",
    "        # State is irrelevant here\n",
    "        state = 5\n",
    "\n",
    "        # Set weights of nn so action 2 has the highest state action value \n",
    "        weights = np.array([np.array([[0., 1., 2., 1.]]), np.array([0., 0., 0. ,0.])], dtype='object')\n",
    "        nn.set_weights(weights)\n",
    "        \n",
    "        self.assertEqual(egreedy_select(state, nn, epsilon=0), 2)\n",
    "\n",
    "    def test_egreedy_select2(self):\n",
    "        '''\n",
    "        Test random selection of the state when all state action values are the same\n",
    "        '''\n",
    "        nn = tf.keras.models.Sequential([\n",
    "            tf.keras.layers.Input(shape=STATE_FEATURES),\n",
    "            tf.keras.layers.Dense(len(ACTIONS))\n",
    "        ])\n",
    "\n",
    "        nn.compile(\n",
    "            optimizer=keras.optimizers.Adam(learning_rate=0.01),\n",
    "            loss=tf.keras.losses.MeanSquaredError()\n",
    "        )\n",
    "\n",
    "        # State is irrelevant here\n",
    "        state = 5\n",
    "\n",
    "        # Set weights \n",
    "        weights = np.array([np.array([[1., 1., 1., 1.]]), np.array([0., 0., 0. ,0.])], dtype='object')\n",
    "        nn.set_weights(weights)\n",
    "        \n",
    "        np.random.seed(66)\n",
    "        self.assertEqual(egreedy_select(state, nn, epsilon=0), 3)\n",
    "\n",
    "    def test_egreedy_select3(self):\n",
    "        '''\n",
    "        Test random selection with epsilon <> 0\n",
    "        '''\n",
    "        nn = tf.keras.models.Sequential([\n",
    "            tf.keras.layers.Input(shape=STATE_FEATURES),\n",
    "            tf.keras.layers.Dense(len(ACTIONS))\n",
    "        ])\n",
    "\n",
    "        nn.compile(\n",
    "            optimizer=keras.optimizers.Adam(learning_rate=0.01),\n",
    "            loss=tf.keras.losses.MeanSquaredError()\n",
    "        )\n",
    "\n",
    "        # State is irrelevant here\n",
    "        state = 5\n",
    "\n",
    "        # Set weights \n",
    "        weights = np.array([np.array([[0., 1., 2., 3.]]), np.array([0., 0., 0. ,0.])], dtype='object')\n",
    "        nn.set_weights(weights)\n",
    "        \n",
    "        np.random.seed(66)\n",
    "        self.assertEqual(egreedy_select(state, nn, epsilon=1), 3)\n",
    "\n",
    "unittest.main(argv=[''], verbosity=2, exit=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
