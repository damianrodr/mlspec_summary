{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement learning\n",
    "\n",
    "## Description of the problem\n",
    "\n",
    "An **entity** can be is a **state** $s$. Each state has associated a **reward** $R(s)$. From that state it can take **actions** $a$ that will take the entity to another state $s'$. The state, its reward, an action that can be taken in that state, and the state the entity will get to with that action can be represented with a tuple:\n",
    "\n",
    "$$(s, a, R(s), s')$$\n",
    "\n",
    "**Return** is the sum of the rewards from a sequence of states and actions, wieghted by a discount factor that compounds along the series of actions. \n",
    "\n",
    "$$return = R(s) + \\gamma R(s') + \\gamma^2 R(s'') + \\dots$$\n",
    "\n",
    "A **Policy** is a function $\\Pi$ that for a given state gives us the action $a$ to take next.\n",
    "\n",
    "The objective of **reinforcement learning algorithm** is to find a policy $\\Pi(s) = a$ that maximizes the return.\n",
    "\n",
    "This problem formulation is a **Markov Decision Process (MDP)**. In a MDP, the future only depends on the current state, regardless of how we have gotten to that state.\n",
    "\n",
    "## State action value function\n",
    "\n",
    "We define the **state action value function** (sometimes also called *Q-function*) $Q(s, a)$ is the return if you,\n",
    "* start in state $s$\n",
    "* take action $a$\n",
    "* behave optimally after that\n",
    "\n",
    "The best possible return from state s is $\\max_{a} Q(s,a)$\n",
    "\n",
    "## Bellman equation\n",
    "\n",
    "$$Q(s, a) = R(s) + \\gamma \\max_{a'} Q(s', a')$$\n",
    "\n",
    "Where $a'$ is each of the actions that can be taken in state $s'$\n",
    "\n",
    "## Random (stochastic) environment\n",
    "\n",
    "In this case the return of a sequence of states is\n",
    "\n",
    "return = $$E[R(s) + \\gamma R(s') + \\gamma^2 R(s'') + \\dots ]$$\n",
    "\n",
    "And the Belmman equation is:\n",
    "\n",
    "$$Q(s,a) = R(s) + \\gamma E[\\max_{a'} Q(s', a')]$$\n",
    "\n",
    "## Continuous state spaces\n",
    "\n",
    "Example: state space for a car, in a 2-D world,\n",
    "\n",
    "$$\\begin{bmatrix}x \\\\ y \\\\ \\theta \\\\ \\dot{x} \\\\ \\dot{y} \\\\ \\dot{\\theta}\\end{bmatrix}$$\n",
    "\n",
    "where $x$ and $y$ are the coordinates, $\\theta$ is the orientation (angle), and $\\dot{x}$, $\\dot{y}$, $\\dot{\\theta}$ the rate of change along each coordinate and the orientation.\n",
    "\n",
    "## Learning the state action value function\n",
    "\n",
    "Idea: Train a neural network to calculate, for a given state $s$, the return of the state action value functions for the actions possible in that state, so we can choose the one with largest $Q(s, a)$. In other words, train a neural network that given s and a returns $y \\approx Q(a,a)$. Or, in less words, train the neural network to learn the Bellman equation.\n",
    "\n",
    "To do so, we can create a large set of tuples\n",
    "\n",
    "$$(s^{(1)}, a^{(1)}, R(s^{(1)}), s'^{(1)}) \\\\ (s^{(2)}, a^{(2)}, R(s^{(2)}), s'^{(2)}) \\\\ \\dots$$\n",
    "\n",
    "And then, the training examples for the neural network will be:\n",
    "\n",
    "* For the inputs $x$, each of the tuples \n",
    "$$(s^{(1)}, a^{(1)}), (s^{(2)}, a^{(2)}), \\dots$$\n",
    "* For the target values y, the corresponding \n",
    "$$Q(s^{(1)},a^{(1)}), Q(s^{(2)},a^{(2)}), \\dots$$\n",
    "\n",
    "calculated with the Bellman equation, for example\n",
    "\n",
    "$$Q(s^{(1)}, a^{(1)}) = R(s^{(1)}) + \\gamma \\max_{a'} Q(s'^{(1)}, a')$$\n",
    "\n",
    "Note that the target values $y$ depend only on the last two elements of the tuples $(s^{(i)}, a^{(i)}, R(s^{(i)}), s'^{(i)})$\n",
    "\n",
    "At the begining, we don't know the $Q(s, a)$ function, but it can be initialized randomly. In every step, it will get better.\n",
    "\n",
    "Learning algorithm (sometimes call the **Deep-Q network**)\n",
    "\n",
    "<pre>\n",
    "    Initialize Q_nn neural network randomly as guess of Q(s, a)\n",
    "    Repeat {\n",
    "        Take actions to generate tuples (s, a, R(s), s')\n",
    "        Store the 10,000 more recent examples of these tuples (replay buffer)\n",
    "        Train neural network:\n",
    "            Create training set of 10,000 examples x, y using\n",
    "                x = (s, a) and y = R(s) + &gamma; max<sub>a'</sub> Q_nn(s', a')\n",
    "            Train Q<sub>new</sub> such that Q<sub>new</sub>(s, a) &asymp; y\n",
    "        Set Q_nn = Q<sub>new</sub>\n",
    "    }\n",
    "</pre>\n",
    "\n",
    "> Note: It is not clear in the lecture, I interpret \"take actions to generate tuples\" in the lesson as to take sequence of actions until you reach a final state. Refer to the ideas in the \"Search\" chapter of the *CS50: AI with Python* course in edX. Maybe not, since here we are just trying to generate training samples to calculate $Q(s, a)$\n",
    "\n",
    "One possible architecture of the neural network is (from course example for lunar lander, with 8 parameters for the state,and 4 possible actions, one hot encoded):\n",
    "\n",
    "```pyhton\n",
    "tf.keras.models.Sequential ([\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dense(1)\n",
    "]) \n",
    "```\n",
    "\n",
    "An improved architecture uses (for this case) four units in the output layer, to compute at the same time the $Q(s, a)$ function for all the possible actions in one state. The input, in this case, is the 8 parameters that represent the state.\n",
    "\n",
    "```pyhton\n",
    "tf.keras.models.Sequential ([\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dense(4)\n",
    "]) \n",
    "```\n",
    "\n",
    "> Note: With this network configuration, the standard way to calculate the loss won't work,as the Bellman equation uses the four outputs. See how it is done in the implementation example bellow.\n",
    "\n",
    "## Algorithm refinement: $\\epsilon$-greedy policy\n",
    "\n",
    "How to improve the \"`Take actions to generate tuples (s, a, R(s), a')`\" step in the algorithm?\n",
    "\n",
    "> Note to self: again, refer to the \"Search\" chapter of the *CS50: AI with Python* course in edX.\n",
    "\n",
    "Instead of taking the actions randomly, use the following algorithm,\n",
    "\n",
    "<pre>\n",
    "    With probability (1- &epsilon;) pick the action a that maximizes Q(s, a)\n",
    "    With probability &epsilon; pick an action a randomly\n",
    "</pre>\n",
    "\n",
    "## Algorithm refinement: mini-batch and soft updates\n",
    "\n",
    "### Mini-bacthes\n",
    "\n",
    "This refinement also applies to linear regression or the training of a neural network.\n",
    "\n",
    "Idea: instead of using all the samples to calculate the cost function in each step of the gradient decent algorithm, do it using a subset (*batch*) of the trainign examples (e.g., with a training set of 1,000,000, use a batch of 1,000 examples).\n",
    "\n",
    "### Soft updates\n",
    "\n",
    "The last step in the algorithm was to replace the $Q(s, a)$ function with the newly calculated $Q_{new}(s, a)$. Doing so, it can create abrupt changes in the Q function, sometings replacing a somehow good function by a worse one.\n",
    "\n",
    "The idea is instead of replacing the parameters of the newural network with the new ones, replace them so that:\n",
    "\n",
    "$$\n",
    "   W = 0.01 W_{new} + 0.99 W \\\\\n",
    "   B = 0.01 B_{new} + 0.99 B\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from collections import deque, namedtuple\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import gym\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "STATE_FEATURES = 1\n",
    "STATES_N = 64\n",
    "ACTIONS = [0, 1, 2, 3]\n",
    "EPSILON = 0.01\n",
    "GAMMA = 0.98\n",
    "ALPHA = 0.001\n",
    "TAU = 0.001\n",
    "\n",
    "#ITERATIONS = 10000\n",
    "#UPDATE_STEP = 500\n",
    "#TRAIN_SAMPLES = 100\n",
    "ITERATIONS = 500\n",
    "UPDATE_STEP = 20\n",
    "TRAIN_SAMPLES = 20\n",
    "\n",
    "DEBUG = True\n",
    "RENDER = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_NN():\n",
    "    \"\"\"\n",
    "    Defines and compiles the neural network\n",
    "    \"\"\"\n",
    "    nnetwork = tf.keras.models.Sequential ([\n",
    "        tf.keras.layers.Input(shape=STATE_FEATURES),\n",
    "        tf.keras.layers.Dense(64, activation=\"relu\"),\n",
    "        tf.keras.layers.Dense(64, activation=\"relu\"),\n",
    "        tf.keras.layers.Dense(len(ACTIONS))\n",
    "    ])\n",
    "\n",
    "    nnetwork.compile(\n",
    "        optimizer=keras.optimizers.Adam(learning_rate=ALPHA),\n",
    "        loss=tf.keras.losses.MeanSquaredError()\n",
    "    )\n",
    "\n",
    "    return nnetwork"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def egreedy_select(state, qnetwork, epsilon):\n",
    "    \"\"\"\n",
    "    Given a state and a state action value function (implemented with a q-network) selects an\n",
    "    action implementing the e-greedy policy\n",
    "\n",
    "    Arguments:\n",
    "        state:\n",
    "        qnetwork:\n",
    "        epsilon:\n",
    "\n",
    "    Returns:\n",
    "        action:\n",
    "    \"\"\"\n",
    "    \n",
    "    if np.random.choice([True, False], p=[1 - epsilon, epsilon]):\n",
    "        # Here the greedy selection of next action\n",
    "        # Transform dimensions of state for qnetwork\n",
    "        state_qn = np.expand_dims(state, axis=0)\n",
    "        q_values = qnetwork(state_qn).numpy()\n",
    "        if np.unique(q_values).shape[0] == 1:\n",
    "            # All q_values are the same, we do a random select\n",
    "            action = np.random.choice(ACTIONS)\n",
    "        else:   \n",
    "            # Selection of the action with max q_value\n",
    "            action = q_values.argmax(axis=1)\n",
    "    else:\n",
    "        action = np.random.choice(ACTIONS)\n",
    "    \n",
    "    return int(action)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss(samples, q_new, q_nn, gamma):\n",
    "\n",
    "    states = tf.convert_to_tensor(\n",
    "        np.array([s.state for s in samples if s is not None]), dtype=tf.float32\n",
    "    )\n",
    "    actions = tf.convert_to_tensor(\n",
    "        np.array([s.action for s in samples if s is not None]), dtype=tf.float32\n",
    "    )\n",
    "    rewards = tf.convert_to_tensor(\n",
    "        np.array([s.reward for s in samples if s is not None]), dtype=tf.float32\n",
    "    )\n",
    "    states_prime = tf.convert_to_tensor(\n",
    "        np.array([s.state_prime for s in samples if s is not None]), dtype=tf.float32\n",
    "    )\n",
    "    done_values = tf.convert_to_tensor(\n",
    "        np.array([s.done for s in samples if s is not None]), dtype=tf.float32\n",
    "    )\n",
    "\n",
    "    # Bellman equation\n",
    "    qvalues_target = rewards + gamma * (1 - done_values) * tf.reduce_max(q_nn(states_prime), axis=-1)\n",
    "\n",
    "    # Get the q_values\n",
    "    q_values = q_new(states)\n",
    "    q_values = tf.gather_nd(q_values, tf.stack([tf.range(q_values.shape[0]),\n",
    "                                                tf.cast(actions, tf.int32)], axis=1))\n",
    "        \n",
    "    return tf.keras.losses.MSE(qvalues_target, q_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@tf.function\n",
    "def train_nn(samples, gamma, optimizer, q_new, q_nn):\n",
    "    \"\"\"\n",
    "    Updates the weights of the Q networks.\n",
    "    \n",
    "    Args:\n",
    "      experiences: (tuple) tuple of [\"state\", \"action\", \"reward\", \"next_state\", \"done\"] namedtuples\n",
    "      gamma: (float) The discount factor.\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    with tf.GradientTape() as tape:\n",
    "        loss =  compute_loss(samples, q_new, q_nn, gamma)\n",
    "\n",
    "    # Get the gradients of the loss with respect to the weights.\n",
    "    gradients = tape.gradient(loss, q_new.trainable_variables)\n",
    "    \n",
    "    # Update the weights of the q_network.\n",
    "    optimizer.apply_gradients(zip(gradients, q_new.trainable_variables))\n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_nn = create_NN()\n",
    "q_new = create_NN()\n",
    "\n",
    "optimizer=keras.optimizers.Adam(learning_rate=ALPHA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if RENDER:\n",
    "    env = gym.make('FrozenLake-v1', desc=None, map_name=\"8x8\", is_slippery=True, render_mode=\"human\")\n",
    "else:\n",
    "    env = gym.make('FrozenLake-v1', desc=None, map_name=\"8x8\", is_slippery=True)\n",
    "state, _ = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# buffer to store the tupples (state, action, reward, state_prime)\n",
    "buffer = deque(maxlen=UPDATE_STEP)\n",
    "\n",
    "# Namedtuple to hold the data points\n",
    "data_point = namedtuple(\"data_point\", [\"state\", \"action\", \"reward\", \"state_prime\", \"done\"])\n",
    "\n",
    "if DEBUG:\n",
    "    # Structure to keep track of visited states and plot a heatmap:\n",
    "    #   Rows: update iteration\n",
    "    #   Columns: states\n",
    "    #   Values: number of visits to each sate\n",
    "    heatmaps = np.zeros((UPDATE_STEP, STATES_N))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(ITERATIONS):\n",
    "\n",
    "    action = egreedy_select(state, q_nn, EPSILON)\n",
    "    state_prime, reward, done, _, _ = env.step(action)\n",
    "\n",
    "    if RENDER:\n",
    "        env.render()\n",
    "    if DEBUG:\n",
    "        heatmaps[(i+1) % UPDATE_STEP, state] += 1\n",
    "\n",
    "    buffer.append(data_point(state, action, reward, state_prime, done))\n",
    "    if done:\n",
    "        # Go back to initial state\n",
    "        state, _ = env.reset()\n",
    "    else:\n",
    "        state = state_prime\n",
    "\n",
    "    if ((i+1) % UPDATE_STEP) == 0:\n",
    "        samples = random.sample(buffer,k=TRAIN_SAMPLES)\n",
    "        \n",
    "        # Train the neural network\n",
    "        train_nn(samples, GAMMA, optimizer, q_new, q_nn)\n",
    "\n",
    "        # update the weights of target q_network\n",
    "        for q_nn_weights, q_new_weights in zip(\n",
    "            q_nn.weights, q_new.weights\n",
    "        ):\n",
    "            q_nn_weights.assign(TAU * q_new_weights + (1.0 - TAU) * q_nn_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if DEBUG:\n",
    "    n_heatmaps = heatmaps.shape[0]\n",
    "    ncols = 4\n",
    "    fig, axs = plt.subplots(\n",
    "        nrows= n_heatmaps // ncols, \n",
    "        ncols=ncols, \n",
    "        figsize=[18,18]\n",
    "    )\n",
    "    for i in range(n_heatmaps):\n",
    "        axs[i // ncols , i % ncols].imshow(np.reshape(heatmaps[i,:], (8,8)), cmap=\"Greens\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if RENDER:\n",
    "    env.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Unit tests\n",
    "\n",
    "import unittest \n",
    "\n",
    "class TestNotebook(unittest.TestCase):\n",
    "\n",
    "    def test_egreedy_select1(self):\n",
    "        '''\n",
    "        Test selection of the state with highest state action value\n",
    "        '''\n",
    "        nn = tf.keras.models.Sequential([\n",
    "            tf.keras.layers.Input(shape=STATE_FEATURES),\n",
    "            tf.keras.layers.Dense(len(ACTIONS))\n",
    "        ])\n",
    "\n",
    "        nn.compile(\n",
    "            optimizer=keras.optimizers.Adam(learning_rate=0.01),\n",
    "            loss=tf.keras.losses.MeanSquaredError()\n",
    "        )\n",
    "\n",
    "        # State is irrelevant here\n",
    "        state = 5\n",
    "\n",
    "        # Set weights of nn so action 2 has the highest state action value \n",
    "        weights = np.array([np.array([[0., 1., 2., 1.]]), np.array([0., 0., 0. ,0.])], dtype='object')\n",
    "        nn.set_weights(weights)\n",
    "        \n",
    "        self.assertEqual(egreedy_select(state, nn, epsilon=0), 2)\n",
    "\n",
    "    def test_egreedy_select2(self):\n",
    "        '''\n",
    "        Test random selection of the state when all state action values are the same\n",
    "        '''\n",
    "        nn = tf.keras.models.Sequential([\n",
    "            tf.keras.layers.Input(shape=STATE_FEATURES),\n",
    "            tf.keras.layers.Dense(len(ACTIONS))\n",
    "        ])\n",
    "\n",
    "        nn.compile(\n",
    "            optimizer=keras.optimizers.Adam(learning_rate=0.01),\n",
    "            loss=tf.keras.losses.MeanSquaredError()\n",
    "        )\n",
    "\n",
    "        # State is irrelevant here\n",
    "        state = 5\n",
    "\n",
    "        # Set weights \n",
    "        weights = np.array([np.array([[1., 1., 1., 1.]]), np.array([0., 0., 0. ,0.])], dtype='object')\n",
    "        nn.set_weights(weights)\n",
    "        \n",
    "        np.random.seed(66)\n",
    "        self.assertEqual(egreedy_select(state, nn, epsilon=0), 3)\n",
    "\n",
    "    def test_egreedy_select3(self):\n",
    "        '''\n",
    "        Test random selection with epsilon <> 0\n",
    "        '''\n",
    "        nn = tf.keras.models.Sequential([\n",
    "            tf.keras.layers.Input(shape=STATE_FEATURES),\n",
    "            tf.keras.layers.Dense(len(ACTIONS))\n",
    "        ])\n",
    "\n",
    "        nn.compile(\n",
    "            optimizer=keras.optimizers.Adam(learning_rate=0.01),\n",
    "            loss=tf.keras.losses.MeanSquaredError()\n",
    "        )\n",
    "\n",
    "        # State is irrelevant here\n",
    "        state = 5\n",
    "\n",
    "        # Set weights \n",
    "        weights = np.array([np.array([[0., 1., 2., 3.]]), np.array([0., 0., 0. ,0.])], dtype='object')\n",
    "        nn.set_weights(weights)\n",
    "        \n",
    "        np.random.seed(66)\n",
    "        self.assertEqual(egreedy_select(state, nn, epsilon=1), 3)\n",
    "\n",
    "unittest.main(argv=[''], verbosity=2, exit=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
