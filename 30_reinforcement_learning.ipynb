{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement learning\n",
    "\n",
    "## Description of the problem\n",
    "\n",
    "An **entity** can be is a **state** $s$. Each state has associated a **reward** $R(s)$. From that state it can take **actions** $a$ that will take the entity to another state $s'$. The state, its reward, an action that can be taken in that state, and the state the entity will get to with that action can be represented with a tuple:\n",
    "\n",
    "$$(s, a, R(s), s')$$\n",
    "\n",
    "**Return** is the sum of the rewards from a sequence of states and actions, wieghted by a discount factor that compounds along the series of actions. \n",
    "\n",
    "$$return = R(s) + \\gamma R(s') + \\gamma^2 R(s'') + \\dots$$\n",
    "\n",
    "A **Policy** is a function $\\Pi$ that for a given state gives us the action $a$ to take next.\n",
    "\n",
    "The objective of **reinforcement learning algorithm** is to find a policy $\\Pi(s) = a$ that maximizes the return.\n",
    "\n",
    "This problem formulation is a **Markov Decision Process (MDP)**. In a MDP, the future only depends on the current state, regardless of how we have gotten to that state.\n",
    "\n",
    "## State action value function\n",
    "\n",
    "We define the **state action value function** (sometimes also called *Q-function*) $Q(s, a)$ is the return if you,\n",
    "* start in state $s$\n",
    "* take action $a$\n",
    "* behave optimally after that\n",
    "\n",
    "The best possible return from state s is $\\max_{a} Q(s,a)$\n",
    "\n",
    "## Bellman equation\n",
    "\n",
    "$$Q(s, a) = R(s) + \\gamma \\max_{a'} Q(s', a')$$\n",
    "\n",
    "Where $a'$ is each of the actions that can be taken in state $s'$\n",
    "\n",
    "## Random (stochastic) environment\n",
    "\n",
    "In this case the return of a sequence of states is\n",
    "\n",
    "return = $$E[R(s) + \\gamma R(s') + \\gamma^2 R(s'') + \\dots ]$$\n",
    "\n",
    "And the Belmman equation is:\n",
    "\n",
    "$$Q(s,a) = R(s) + \\gamma E[\\max_{a'} Q(s', a')]$$\n",
    "\n",
    "## Continuous state spaces\n",
    "\n",
    "Example: state space for a car, in a 2-D world,\n",
    "\n",
    "$$\\begin{bmatrix}x \\\\ y \\\\ \\theta \\\\ \\dot{x} \\\\ \\dot{y} \\\\ \\dot{\\theta}\\end{bmatrix}$$\n",
    "\n",
    "where $x$ and $y$ are the coordinates, $\\theta$ is the orientation (angle), and $\\dot{x}$, $\\dot{y}$, $\\dot{\\theta}$ the rate of change along each coordinate and the orientation.\n",
    "\n",
    "## Learning the state action value function\n",
    "\n",
    "Idea: Train a neural network to calculate, for a given state $s$, the return of the state action value functions for the actions possible in that state, so we can choose the one with largest $Q(s, a)$. In other words, train a neural network that given s and a returns $y \\approx Q(a,a)$. Or, in less words, train the neural network to learn the Bellman equation.\n",
    "\n",
    "To do so, we can create a large set of tuples\n",
    "\n",
    "$$(s^{(1)}, a^{(1)}, R(s^{(1)}), s'^{(1)}) \\\\ (s^{(2)}, a^{(2)}, R(s^{(2)}), s'^{(2)}) \\\\ \\dots$$\n",
    "\n",
    "And then, the training examples for the neural network will be:\n",
    "\n",
    "* For the inputs $x$, each of the tuples \n",
    "$$(s^{(1)}, a^{(1)}), (s^{(2)}, a^{(2)}), \\dots$$\n",
    "* For the target values y, the corresponding \n",
    "$$Q(s^{(1)},a^{(1)}), Q(s^{(2)},a^{(2)}), \\dots$$\n",
    "\n",
    "calculated with the Bellman equation, for example\n",
    "\n",
    "$$Q(s^{(1)}, a^{(1)}) = R(s^{(1)}) + \\gamma \\max_{a'} Q(s'^{(1)}, a')$$\n",
    "\n",
    "Note that the target values $y$ depend only on the last two elements of the tuples $(s^{(i)}, a^{(i)}, R(s^{(i)}), s'^{(i)})$\n",
    "\n",
    "At the begining, we don't know the $Q(s, a)$ function, but it can be initialized randomly. In every step, it will get better.\n",
    "\n",
    "Learning algorithm (sometimes call the **Deep-Q network**)\n",
    "\n",
    "<pre>\n",
    "    Initialize neural network randomly as guess of Q(s, a)\n",
    "    Repeat {\n",
    "        Take actions to generate tuples (s, a, R(s), s')\n",
    "        Store the 10,000 more recent examples of these tuples (replay buffer)\n",
    "        Train neural network:\n",
    "            Create training set of 10,000 examples x, y using\n",
    "                x = (s, a) and y = R(s) + &gamma; max<sub>a'</sub> Q(s', a')\n",
    "            Train Q<sub>new</sub> such that Q<sub>new</sub>(s, a) &asymp; y\n",
    "        Set Q = Q<sub>new</sub>\n",
    "    }\n",
    "</pre>\n",
    "\n",
    "> Note: It is not clear in the lecture, but wonder if the \"take actions to generate tuples\" in the lesson means take sequence of actions until you reach a final state. Refer to the ideas in the \"Search\" chapter of the *CS50: AI with Python* course in edX. Maybe not, since here we are just trying to generate training samples to calculate $Q(s, a)$\n",
    "\n",
    "One possible architecture of the neural network is (from course example for lunar lander, with 8 parameters for the state,and 4 possible actions, one hot encoded):\n",
    "\n",
    "```pyhton\n",
    "tf.keras.models.Sequential ([\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dense(1)\n",
    "]) \n",
    "```\n",
    "\n",
    "An improved architecture uses (for this case) four units in the output layer, to compute at the same time the $Q(s, a)$ function for all the possible actions in one state. The input, in this case, is the 8 parameters that represent the state.\n",
    "\n",
    "```pyhton\n",
    "tf.keras.models.Sequential ([\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dense(4)\n",
    "]) \n",
    "```\n",
    "\n",
    "## Algorithm refinement: $\\epsilon$-greedy policy\n",
    "\n",
    "How to improve the \"`Take actions to generate tuples (s, a, R(s), a')`\" step in the algorithm?\n",
    "\n",
    "> Note to self: again, refer to the \"Search\" chapter of the *CS50: AI with Python* course in edX.\n",
    "\n",
    "Instead of taking the actions randomly, use the following algorithm,\n",
    "\n",
    "<pre>\n",
    "    With probability (1- &epsilon;) pick the action a that maximizes Q(s, a)\n",
    "    With probability &epsilon; pick an action a randomly\n",
    "</pre>\n",
    "\n",
    "## Algorithm refinement: mini-batch and soft updates\n",
    "\n",
    "### Mini-bacthes\n",
    "\n",
    "This refinement also applies to linear regression or the training of a neural network.\n",
    "\n",
    "Idea: instead of using all the samples to calculate the cost function in each step of the gradient decent algorithm, do it using a subset (*batch*) of the trainign examples (e.g., with a training set of 1,000,000, use a batch of 1,000 examples).\n",
    "\n",
    "### Soft updates\n",
    "\n",
    "The last step in the algorithm was to replace the $Q(s, a)$ function with the newly calculated $Q_{new}(s, a)$. Doing so, it can create abrupt changes in the Q function, sometings replacing a somehow good function by a worse one.\n",
    "\n",
    "The idea is instead of replacing the parameters of the newural network with the new ones, replace them so that:\n",
    "\n",
    "$$\n",
    "   W = 0.01 W_{new} + 0.99 W \\\\\n",
    "   B = 0.01 B_{new} + 0.99 B\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-08 21:31:38.330062: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX512_VNNI\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "ITERATIONS = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('FrozenLake-v1', desc=None, map_name=\"8x8\", is_slippery=True, render_mode=\"human\")\n",
    "\n",
    "STATE_FEATURES = 1\n",
    "POSSIBLE_ACTIONS = 4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "state, reward, terminated, truncated, info = env.step(env.action_space.sample())\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_3 (Dense)             (None, 64)                128       \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 64)                4160      \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 4)                 260       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 4,548\n",
      "Trainable params: 4,548\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "nnetwork = tf.keras.models.Sequential ([\n",
    "    tf.keras.layers.Input(shape=STATE_FEATURES),\n",
    "    tf.keras.layers.Dense(64, activation=\"relu\"),\n",
    "    tf.keras.layers.Dense(64, activation=\"relu\"),\n",
    "    tf.keras.layers.Dense(POSSIBLE_ACTIONS)\n",
    "])\n",
    "\n",
    "nnetwork.summary()\n",
    "\n",
    "nnetwork.compile(\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=0.01),\n",
    "    loss=tf.keras.losses.MeanSquaredError()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "buffer = []\n",
    "state, _ = env.reset(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/damian/learning/mlspecialization/my_summary/30_reinforcement_learning.ipynb Cell 8\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/home/damian/learning/mlspecialization/my_summary/30_reinforcement_learning.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m action \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39maction_space\u001b[39m.\u001b[39msample()\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/home/damian/learning/mlspecialization/my_summary/30_reinforcement_learning.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m state_prime, reward, done, _, _ \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39mstep(action)\n\u001b[0;32m----> <a href='vscode-notebook-cell://wsl%2Bubuntu/home/damian/learning/mlspecialization/my_summary/30_reinforcement_learning.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m env\u001b[39m.\u001b[39;49mrender()\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/home/damian/learning/mlspecialization/my_summary/30_reinforcement_learning.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m buffer\u001b[39m.\u001b[39mappend((state, action, reward, state_prime))\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/home/damian/learning/mlspecialization/my_summary/30_reinforcement_learning.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39mif\u001b[39;00m done:\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/home/damian/learning/mlspecialization/my_summary/30_reinforcement_learning.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m     \u001b[39m# Go back to initial state\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/gym/core.py:329\u001b[0m, in \u001b[0;36mWrapper.render\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    325\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrender\u001b[39m(\n\u001b[1;32m    326\u001b[0m     \u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs\n\u001b[1;32m    327\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Optional[Union[RenderFrame, List[RenderFrame]]]:\n\u001b[1;32m    328\u001b[0m     \u001b[39m\"\"\"Renders the environment.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 329\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv\u001b[39m.\u001b[39;49mrender(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/gym/wrappers/order_enforcing.py:51\u001b[0m, in \u001b[0;36mOrderEnforcing.render\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_disable_render_order_enforcing \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_has_reset:\n\u001b[1;32m     47\u001b[0m     \u001b[39mraise\u001b[39;00m ResetNeeded(\n\u001b[1;32m     48\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mCannot call `env.render()` before calling `env.reset()`, if this is a intended action, \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     49\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mset `disable_render_order_enforcing=True` on the OrderEnforcer wrapper.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     50\u001b[0m     )\n\u001b[0;32m---> 51\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv\u001b[39m.\u001b[39;49mrender(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/gym/wrappers/env_checker.py:55\u001b[0m, in \u001b[0;36mPassiveEnvChecker.render\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[39mreturn\u001b[39;00m env_render_passive_checker(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39menv, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m     54\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 55\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv\u001b[39m.\u001b[39;49mrender(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/gym/envs/toy_text/frozen_lake.py:279\u001b[0m, in \u001b[0;36mFrozenLakeEnv.render\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    277\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_render_text()\n\u001b[1;32m    278\u001b[0m \u001b[39melse\u001b[39;00m:  \u001b[39m# self.render_mode in {\"human\", \"rgb_array\"}:\u001b[39;00m\n\u001b[0;32m--> 279\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_render_gui(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrender_mode)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/gym/envs/toy_text/frozen_lake.py:373\u001b[0m, in \u001b[0;36mFrozenLakeEnv._render_gui\u001b[0;34m(self, mode)\u001b[0m\n\u001b[1;32m    371\u001b[0m     pygame\u001b[39m.\u001b[39mevent\u001b[39m.\u001b[39mpump()\n\u001b[1;32m    372\u001b[0m     pygame\u001b[39m.\u001b[39mdisplay\u001b[39m.\u001b[39mupdate()\n\u001b[0;32m--> 373\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mclock\u001b[39m.\u001b[39;49mtick(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmetadata[\u001b[39m\"\u001b[39;49m\u001b[39mrender_fps\u001b[39;49m\u001b[39m\"\u001b[39;49m])\n\u001b[1;32m    374\u001b[0m \u001b[39melif\u001b[39;00m mode \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mrgb_array\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m    375\u001b[0m     \u001b[39mreturn\u001b[39;00m np\u001b[39m.\u001b[39mtranspose(\n\u001b[1;32m    376\u001b[0m         np\u001b[39m.\u001b[39marray(pygame\u001b[39m.\u001b[39msurfarray\u001b[39m.\u001b[39mpixels3d(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwindow_surface)), axes\u001b[39m=\u001b[39m(\u001b[39m1\u001b[39m, \u001b[39m0\u001b[39m, \u001b[39m2\u001b[39m)\n\u001b[1;32m    377\u001b[0m     )\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for _ in range(ITERATIONS):\n",
    "    action = env.action_space.sample()\n",
    "    state_prime, reward, done, _, _ = env.step(action)\n",
    "    env.render()\n",
    "    buffer.append((state, action, reward, state_prime))\n",
    "    if done:\n",
    "        # Go back to initial state\n",
    "        state = env.reset()\n",
    "    else:\n",
    "        state = state_prime\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
