{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recommender systems - collaborative filtering\n",
    "\n",
    "## Notation\n",
    "\n",
    "Number of users: $n_u$\n",
    "\n",
    "Number of items: $n_m$ (e.g. movies in the database)\n",
    "\n",
    "Number of features: $n$\n",
    "\n",
    "There is / there isn't a rating: $r(i, j) = 1$ if user $j$ has rated item $i$\n",
    "\n",
    "Ratings: $y^{(i, j)}$: rating of item $i$ given by user $j$\n",
    "\n",
    "## Using per-item features\n",
    "\n",
    "Items are assigned $n$ features.\n",
    "\n",
    "The ratings (preditions) are defined as a linear function of the features, defined user by user.\n",
    "\n",
    "Predicted rating given to item $i$ by user $j$: \n",
    "\n",
    "$$y^{(i, j)} = w^{(j)} \\cdot x^{(i)} + b^{(j)}$$\n",
    "\n",
    "### Cost function\n",
    "\n",
    "$m^{(j)}$: number of items rated by user $j$\n",
    "\n",
    "Cost function (with regularization) for one user $j$:\n",
    "\n",
    "$$\\min_{w^{(j)}b^{(j)}} J(w^{(j)}, b^{(j)}) = \\frac{1}{2}\\sum_{i:r(i,j)=1}(w^{(j)} \\cdot x^{(i)}+b^{(j)}-y^{(i, j)})^2 + \\frac{\\lambda}{2}\\sum^n_{k=1}(w_k^{(j)})^2$$\n",
    "\n",
    "Cost function for all users at once:\n",
    "\n",
    "$$\\min_{\\substack{w^{(1)}, \\dots, w^{(n_u)}\\\\b^{(1)}, \\dots, b^{(n_u)}}} J(\\substack{w^{(1)}\\dots w^{(n_u)}\\\\b^{(1)}\\dots b^{(n_u)}}) = \\frac{1}{2}\\sum_{j=1}^{n_u}\\sum_{i:r(i,j)=1}(w^{(j)} \\cdot x^{(i)}+b^{(j)}-y^{(i, j)})^2 + \\frac{\\lambda}{2}\\sum_{j=1}^{n_u}\\sum^n_{k=1}(w_k^{(j)})^2$$\n",
    "\n",
    "## Collaborative filtering algorithm (i.e., without per-item features)\n",
    "\n",
    "The algorithm has to find the parameters of the linear regressions and <u>the features</u>. Therefore,\n",
    "\n",
    "### Non-binary labels\n",
    "\n",
    "#### Cost function\n",
    "\n",
    "$$\\min_{\\substack{w^{(1)}\\dots w^{(n_u)}\\\\b^{(1)}\\dots b^{(n_u)}\\\\x^{(1)}\\dots x^{(n_m)}}} J(w, b, x) = \\frac{1}{2}\\sum_{(i,j):r(i,j)=1}(w^{(j)} \\cdot x^{(i)}+b^{(j)}-y^{(i, j)})^2 + \\frac{\\lambda}{2}\\sum_{j=1}^{n_u}\\sum^n_{k=1}(w_k^{(j)})^2 + \\frac{\\lambda}{2}\\sum_{i=1}^{n_m}\\sum^n_{k=1}(x_k^{(i)})^2$$\n",
    "\n",
    "### Binary labels (favs, likes, and clicks)\n",
    "\n",
    "Instead of the user giving a rating (e.g., from 0 to 5) it gives or we assign a binary label.\n",
    "\n",
    "In this case, instead of predicting $y^{(i, j)} = w^{(j)} \\cdot x^{(i)} + b^{(j)}$ we are predicting the probability of $y^{(i, j)}$ being 1.\n",
    "\n",
    "$$p(y^{(i,j)} = 1) = g(w^{(j)} \\cdot x^{(i)} + b^{(j)}) \\\\ \\text{ where } g(z) = \\frac{1}{1 + e^{-z}}$$\n",
    "\n",
    "#### Cost function\n",
    "\n",
    "Loss for a single sample:\n",
    "\n",
    "$$f_{(w, b, x)} = g(w^{(j)} \\cdot x^{(i)} + b^{(j)})$$ \n",
    "\n",
    "$$Loss(f_{(w, b, x)}(x), y^{(i,j)}) = -y^{(i,j)} \\log(f_{(w, b, x)}(x)) - (1 - y^{(i,j)}) \\log(1 - f_{(w, b, x)}(x))$$\n",
    "\n",
    "Cost function:\n",
    "\n",
    "$$J(w,b,x) = \\sum_{(i,j):r(i,j)=1} Loss(f_{(w, b, x)}(x), y^{(i,j)})$$\n",
    "\n",
    "## Implementation details\n",
    "\n",
    "### Mean normalization\n",
    "\n",
    "The idea is to normalize the ratings with the means (across each item) and make the predictions be:\n",
    "\n",
    "$$w^{(j)} \\cdot x^{(i)} + b^{(j)} + \\mu_i$$\n",
    "\n",
    "Implicitly making the prediction for a non rated item equal to the mean.\n",
    "\n",
    "### Implementation in TensorFlow\n",
    "\n",
    "Simplified example, $$J = (wx -1)^2$$\n",
    "\n",
    "Algorithm:\n",
    "\n",
    "<pre>\n",
    "Repeat until convergence:\n",
    "</pre>\n",
    "&nbsp;&nbsp;&nbsp; $w = w - \\alpha \\frac{\\partial}{\\partial w}J(w,b)$\n",
    "\n",
    "With TensorFlow:\n",
    "\n",
    "``` python\n",
    "w = tf.Variable(3.0)\n",
    "x = 1.0\n",
    "y = 1.0 # Target value\n",
    "alpha = 0.1\n",
    "\n",
    "iterations = 30\n",
    "for iter in range(iterations):\n",
    "    # Use TensorFLow's Gradient tape to record the steps\n",
    "    # used to compute the cosr L to enable auto differentiation\n",
    "    with tf.GradientTape() as tape:\n",
    "        fwb = w*x\n",
    "        costJ = (fwb - y)^2\n",
    "\n",
    "    # Use the gradient tape to calculate the gradients\n",
    "    # of the cost with respect to the parameter w\n",
    "    [dJdw] = tape.gradient(costJ, [w])\n",
    "\n",
    "    # Run one step gradient descent by updating\n",
    "    # the value of w to reduce the cost\n",
    "    w.assign_add(-alpha * dJdw)\n",
    "```\n",
    "\n",
    "### Finding related items\n",
    "\n",
    "To find the item with $x^{(k)}$ more similar to $x^{(i)}$, once you have run the algorithm above, and you have determined the features $x^{(i)}$, you look for the item closer to $x^{(i)}$\n",
    "\n",
    "$$\\left\\Vert x^{(k)} - x^{(i)}\\right\\Vert^2 = \\sum_{l=1}^n (x^{(k)}_l - x^{(i)}_l)^2$$\n",
    "\n",
    "### Limitations\n",
    "\n",
    "* Cold start:\n",
    "    * Rank new items that few users have rated yet\n",
    "    * Make recomendations to users who have rated few items (although mean normalization helps)\n",
    "\n",
    "* Does not factor in information (features) already know about the users or the items (e.g. preferences)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python, vectorized implementation\n",
    "\n",
    "Terminology:\n",
    "\n",
    "Number of users: $n_u$\n",
    "\n",
    "Number of items: $n_m$ \n",
    "\n",
    "Number of features: $n$\n",
    "\n",
    "\n",
    "Matrix representations:\n",
    "\n",
    "* X is the matrix of features, $n_m$ x $n$ where row i represents the features of item i\n",
    "\n",
    "$$X = \\begin{bmatrix} ---(x^{(0)})^T--- \\\\ \\vdots \\\\ ---(x^{(n_m - 1)})^T--- \\end{bmatrix}$$\n",
    "\n",
    "* W is the matrix of parameters, $n_u$ x $n$ where row j represents the parameters for user j\n",
    "\n",
    "$$W = \\begin{bmatrix} ---(w^{(0)})^T--- \\\\ \\vdots \\\\ ---(w^{(n_u -1)})^T--- \\end{bmatrix}$$\n",
    "\n",
    "* b is the vector of biases, $n_u$\n",
    "\n",
    "$$b = \\begin{bmatrix} b^{(0)} \\\\ \\vdots \\\\ b^{(n_u -1)} \\end{bmatrix}$$\n",
    "\n",
    "* Y is the matrix of ratings, $n_m$ x $n_u$\n",
    "\n",
    "$$Y = \\begin{bmatrix} y_{0, 0} \\dots y_{0, n_{n_u-1}} \\\\ \\vdots \\\\ y_{n_{m-1}, 0} \\dots y_{n_{(m-1)}, n_{n_u-1}} \\end{bmatrix}$$\n",
    "\n",
    "* R is a flag matrix telling if user $j$ has rated item $i$, $n_m$ x $n_u$ \n",
    "\n",
    "$$R = \\begin{bmatrix} r_{0, 0} \\dots r_{0, n_{n_u-1}} \\\\ \\vdots \\\\ r_{n_{m-1}, 0} \\dots r_{n_{(m-1)}, n_{n_u-1}} \\end{bmatrix}$$\n",
    "\n",
    "Predictions:\n",
    "\n",
    "$$ \\hat{Y} = X * W^T + b^T$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-24 19:19:02.600687: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX512_VNNI\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cofi_cost_function(X, W, b, Y, R, lambda_):\n",
    "    \"\"\"\n",
    "    Cost function for collaborative filtering\n",
    "    Args:\n",
    "      X (ndarray (n_m, n)) : item features\n",
    "      W (ndarray (n_u, n)) : user parameters\n",
    "      b (ndarray (n_u, 1)  : user bias parameters\n",
    "      Y (ndarray (n_m, n_u): user ratings\n",
    "      R (ndarray (n_m, n_u): R(i, j) = 1 if the i-th item was rated by the j-th user\n",
    "      lambda_ (float)      : regularization parameter\n",
    "    Returns:\n",
    "      J (float) : Cost\n",
    "    \"\"\"\n",
    "\n",
    "    return 0.5*np.sum((((np.matmul(X, W.T) + b.T) - Y)*R) ** 2) + (lambda_/2)*(np.sum(W**2) + np.sum(X**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cofi_cost_function_tf(X, W, b, Y, R, lambda_):\n",
    "    \"\"\"\n",
    "    Cost function for collaborative filtering, implemented using TensorFlow\n",
    "    Args:\n",
    "      X (ndarray (n_m, n)) : item features\n",
    "      W (ndarray (n_u, n)) : user parameters\n",
    "      b (ndarray (n_u, 1)  : user bias parameters\n",
    "      Y (ndarray (n_m, n_u): user ratings\n",
    "      R (ndarray (n_m, n_u): R(i, j) = 1 if the i-th item was rated by the j-th user\n",
    "      lambda_ (float)      : regularization parameter\n",
    "    Returns:\n",
    "      J (float) : Cost\n",
    "    \"\"\"\n",
    "    j = (tf.linalg.matmul(X, tf.transpose(W)) + tf.transpose(b) - Y)*R\n",
    "    J = 0.5 * tf.reduce_sum(j**2) + (lambda_/2) * (tf.reduce_sum(X**2) + tf.reduce_sum(W**2))\n",
    "    return J"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommender(Y_, R_, n, lambda_=1.0, iterations=200):\n",
    "    '''\n",
    "    Implements a collaborative recommender system with TensorFlow\n",
    "    Args:\n",
    "      Y_ (ndarray (n_m, n_u)  : user ratings\n",
    "      R_ (ndarray (n_m, n_u)  : R(i, j) = 1 if the i-th item was rated by the j-th user\n",
    "      n (int)                 : number of item features to extract\n",
    "      lambda_ (float)         : regularization parameter\n",
    "      iterations (float)      : number of iterations in the gradient descent optimization\n",
    "    Returns:\n",
    "      X (ndarray (n_m, n))    : item features\n",
    "      W (ndarray (n_u, n))    : user parameters\n",
    "      b (ndarray (n_u, 1)     : user bias parameters\n",
    "    '''\n",
    "\n",
    "    # Some useful values\n",
    "\n",
    "    n_m, n_u = Y_.shape\n",
    "\n",
    "    # Convert to tensorflow constant\n",
    "\n",
    "    R = tf.constant(R_, dtype=tf.float64, name='R')\n",
    "    Y = tf.constant(Y_, dtype=tf.float64, name='Y')\n",
    "\n",
    "    # Initialize parameters\n",
    "\n",
    "    tf.random.set_seed(1)\n",
    "\n",
    "    W = tf.Variable(tf.random.normal((n_u, n), dtype=tf.float64), name='W')\n",
    "    X = tf.Variable(tf.random.normal((n_m, n), dtype=tf.float64), name='X')\n",
    "    b = tf.Variable(tf.random.normal((n_u, 1), dtype=tf.float64), name='b')\n",
    "\n",
    "    # Instantiate an optimizer\n",
    "\n",
    "    optimizer = keras.optimizers.Adam(learning_rate=0.1)\n",
    "\n",
    "    # Optimization\n",
    "\n",
    "    for iter in range(iterations):\n",
    "        with tf.GradientTape() as tape:\n",
    "            # Compute cost\n",
    "            cost_value = cofi_cost_function_tf(X, W, b, Y, R, lambda_)        \n",
    "\n",
    "        grads = tape.gradient(cost_value, [X, W, b])\n",
    "\n",
    "        optimizer.apply_gradients(zip(grads, [X, W, b]))\n",
    "\n",
    "        if iter % 20 == 0:\n",
    "            print(f\"Training loss at iteration {iter}: {cost_value:,.1f}\")\n",
    "\n",
    "    return X.numpy(), W.numpy(), b.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num items (movies), n_m: 1297\n",
      "Num users, n_u: 610\n",
      "Num features, n: 100\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Read data: copy of the https://grouplens.org/datasets/movielens/latest/ dataset\n",
    "\n",
    "movies = pd.read_csv(\n",
    "    \"data/MovieLens/movies.csv\",\n",
    "    index_col='movieId')\n",
    "\n",
    "ratings = pd.read_csv(\n",
    "    'data/MovieLens/ratings.csv', \n",
    "    usecols=['userId', 'movieId', 'rating']\n",
    ").pivot_table(\n",
    "    values=['rating'], \n",
    "    columns= ['userId'], \n",
    "    index=['movieId']\n",
    ").droplevel(\n",
    "    0, \n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Drop movies with few ratings (less than 20). \n",
    "\n",
    "ratings = ratings.loc[Y_df.count(axis=1) >= 20].dropna(axis=1, how='all')\n",
    "\n",
    "Y_raw = ratings.to_numpy()\n",
    "\n",
    "n_m = Y_raw.shape[0]    # Number of items (movies)\n",
    "n_u = Y_raw.shape[1]    # Number of users\n",
    "n = 100                 # Number of features we will use\n",
    "\n",
    "print(f\"Num items (movies), n_m: {n_m}\")\n",
    "print(f\"Num users, n_u: {n_u}\")\n",
    "print(f\"Num features, n: {n}\")\n",
    "\n",
    "#_, _, _ = plt.hist(Y_raw[1,:])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize ratings, nan converted to 0 after calculating the mean\n",
    "Y_mean = np.nanmean(Y_raw, axis=1, keepdims=True)\n",
    "Y = np.nan_to_num(Y_raw - Y_mean)\n",
    "\n",
    "# Calculate R\n",
    "R = (~np.isnan(Y_raw)).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-24 19:19:28.398692: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX512_VNNI\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-02-24 19:19:28.774549: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss at iteration 0: 3,554,636.9\n",
      "Training loss at iteration 20: 102,161.4\n",
      "Training loss at iteration 40: 46,436.3\n",
      "Training loss at iteration 60: 28,831.6\n",
      "Training loss at iteration 80: 19,507.8\n",
      "Training loss at iteration 100: 13,930.0\n",
      "Training loss at iteration 120: 10,416.6\n",
      "Training loss at iteration 140: 8,153.9\n",
      "Training loss at iteration 160: 6,672.9\n",
      "Training loss at iteration 180: 5,688.0\n"
     ]
    }
   ],
   "source": [
    "# Run recommender\n",
    "\n",
    "X, W, b = recommender(Y, R, n, lambda_=1, iterations = 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict\n",
    "\n",
    "p = np.matmul(X, W.T) +b.T + Y_mean\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: 3.981132887522727, actual: nan, mean for movie 0: [3.92093023]\n",
      "Prediction: 3.9921912562038875, actual: 4.0, mean for movie 0: [3.92093023]\n"
     ]
    }
   ],
   "source": [
    "# Movies not rated are assigned the mean\n",
    "\n",
    "print(f\"Prediction: {p[0,1]}, actual: {Y_raw[0,1]}, mean for movie 0: {Y_mean[0]}\")\n",
    "print(f\"Prediction: {p[0,4]}, actual: {Y_raw[0,4]}, mean for movie 0: {Y_mean[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# This calculation excludes non rated items. Error is calculated in terms of half points\n",
    "error = np.round(np.divide(p - Y_raw, 0.5))*0.5\n",
    "\n",
    "_, _, _ = plt.hist(error.reshape(-1))\n",
    "\n",
    "# Mean error by user\n",
    "#_, _, _ = plt.hist(np.nanmean(error, axis=0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision (% of predictions within +/- of original rating): 1.0\n"
     ]
    }
   ],
   "source": [
    "# Precision\n",
    "print(f\"Precision (% of predictions within +/- of original rating): {np.sum(np.abs(error) <= 0.5)/np.nansum(~np.isnan(Y_raw)):0.4}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>genres</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>movieId</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>588</th>\n",
       "      <td>Aladdin (1992)</td>\n",
       "      <td>Adventure|Animation|Children|Comedy|Musical</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3698</th>\n",
       "      <td>Running Man, The (1987)</td>\n",
       "      <td>Action|Sci-Fi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Four Rooms (1995)</td>\n",
       "      <td>Comedy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1688</th>\n",
       "      <td>Anastasia (1997)</td>\n",
       "      <td>Adventure|Animation|Children|Drama|Musical</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>Muppet Treasure Island (1996)</td>\n",
       "      <td>Adventure|Children|Comedy|Musical</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1285</th>\n",
       "      <td>Heathers (1989)</td>\n",
       "      <td>Comedy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8622</th>\n",
       "      <td>Fahrenheit 9/11 (2004)</td>\n",
       "      <td>Documentary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5283</th>\n",
       "      <td>National Lampoon's Van Wilder (2002)</td>\n",
       "      <td>Comedy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45720</th>\n",
       "      <td>Devil Wears Prada, The (2006)</td>\n",
       "      <td>Comedy|Drama</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88405</th>\n",
       "      <td>Friends with Benefits (2011)</td>\n",
       "      <td>Comedy|Romance</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        title  \\\n",
       "movieId                                         \n",
       "588                            Aladdin (1992)   \n",
       "3698                  Running Man, The (1987)   \n",
       "18                          Four Rooms (1995)   \n",
       "1688                         Anastasia (1997)   \n",
       "107             Muppet Treasure Island (1996)   \n",
       "1285                          Heathers (1989)   \n",
       "8622                   Fahrenheit 9/11 (2004)   \n",
       "5283     National Lampoon's Van Wilder (2002)   \n",
       "45720           Devil Wears Prada, The (2006)   \n",
       "88405            Friends with Benefits (2011)   \n",
       "\n",
       "                                              genres  \n",
       "movieId                                               \n",
       "588      Adventure|Animation|Children|Comedy|Musical  \n",
       "3698                                   Action|Sci-Fi  \n",
       "18                                            Comedy  \n",
       "1688      Adventure|Animation|Children|Drama|Musical  \n",
       "107                Adventure|Children|Comedy|Musical  \n",
       "1285                                          Comedy  \n",
       "8622                                     Documentary  \n",
       "5283                                          Comedy  \n",
       "45720                                   Comedy|Drama  \n",
       "88405                                 Comedy|Romance  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Movies closer to ...\n",
    "\n",
    "movieId = movies.index[movies['title'] == 'Aladdin (1992)'].values[0]\n",
    "#movieId = movies.index[movies['title'] == 'Four Weddings and a Funeral (1994)'].values[0]\n",
    "#movieId = movies.index[movies['title'] == 'Star Trek: The Motion Picture (1979)'].values[0]\n",
    "row_num = ratings.index.get_loc(movieId)\n",
    "\n",
    "distances = np.sum((X - X[row_num])**2, axis=1)\n",
    "\n",
    "ratings['dist'] = distances.tolist()\n",
    "\n",
    "# Add count of ratings to each movie. \n",
    "ratings['num_ratings'] = np.nansum(np.divide(Y_raw, Y_raw), axis=1)\n",
    "\n",
    "# Closer movies. Selection is done out of movies with enough number of ratings. Low rated movies would otherwise \n",
    "# will have a high chance to show in the list.\n",
    "movies.loc[ratings.loc[ratings['num_ratings'] >= 20].sort_values('dist', ascending=True).head(10).index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------\n",
      "Ran 3 tests in 1.813s\n",
      "\n",
      "OK\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<unittest.main.TestProgram at 0x7f8a6c0dd370>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Unit tests\n",
    "\n",
    "import unittest \n",
    "\n",
    "class TestNotebook(unittest.TestCase):\n",
    "\n",
    "    def test_cofi_cost_function_no_reg(self):\n",
    "        X = np.array([[1, 2, 3, 4], [5, 6, 7, 8]])\n",
    "        W = np.array([[0.5, 0.2, 0.1, 0.2], [0.1, 0.1, 0.2, 0.2], [0.3, 0.4, 0.1, 0.1]])\n",
    "        b = np.array([[0.1], [0.2], [0.5]])\n",
    "        Y = np.array([[2, 3, 5], [1, 2, 3]])\n",
    "        R = np.array([[1, 1, 1], [1, 1, 1]])\n",
    "        \n",
    "        self.assertEqual(cofi_cost_function(X,W, b, Y, R, lambda_=0), 24.11)\n",
    "\n",
    "    def test_cofi_cost_function_reg(self):\n",
    "        X = np.array([[1, 2, 3, 4], [5, 6, 7, 8]])\n",
    "        W = np.array([[0.5, 0.2, 0.1, 0.2], [0.1, 0.1, 0.2, 0.2], [0.3, 0.4, 0.1, 0.1]])\n",
    "        b = np.array([[0.1], [0.2], [0.5]])\n",
    "        Y = np.array([[2, 3, 5], [1, 2, 3]])\n",
    "        R = np.array([[1, 1, 1], [1, 1, 1]])\n",
    "        \n",
    "        self.assertEqual(cofi_cost_function(X,W, b, Y, R, lambda_=0.1), 34.3455)\n",
    "\n",
    "    def test_cofi_cost_function_tf_reg(self):\n",
    "        X = tf.constant([[1., 2., 3., 4.], [5., 6., 7., 8.]])\n",
    "        W = tf.constant([[0.5, 0.2, 0.1, 0.2], [0.1, 0.1, 0.2, 0.2], [0.3, 0.4, 0.1, 0.1]])\n",
    "        b = tf.constant([[0.1], [0.2], [0.5]])\n",
    "        Y = tf.constant([[2., 3., 5.], [1., 2., 3.]])\n",
    "        R = tf.constant([[1., 1., 1.], [1., 1., 1.]])\n",
    "        \n",
    "        self.assertAlmostEqual(cofi_cost_function_tf(X,W, b, Y, R, lambda_=0.1).numpy(), 34.3455, 3)\n",
    "\n",
    "\n",
    "unittest.main(argv=[''], verbosity=0, exit=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
